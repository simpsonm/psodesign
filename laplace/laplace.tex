\documentclass[12pt]{article}
\setlength{\oddsidemargin}{-0.125in}
\setlength{\topmargin}{-0.5in} \setlength{\textwidth}{6.5in}
\setlength{\textheight}{9in}

\setlength{\textheight}{9in} \setlength{\textwidth}{6.5in}
\setlength{\topmargin}{-40pt} \setlength{\oddsidemargin}{0pt}
\setlength{\evensidemargin}{0pt}

\setlength{\textheight}{8.5in} \setlength{\textwidth}{6.5in}
\setlength{\topmargin}{-36pt} \setlength{\oddsidemargin}{0pt}
\setlength{\evensidemargin}{0pt} \tolerance=500
\renewcommand{\baselinestretch}{1.5}

\usepackage{amssymb, amsmath, latexsym, array, morefloats, epsfig, rotating, graphicx}
\usepackage{subfigure, url, mathtools, enumerate, wasysym, threeparttable, lscape}
\usepackage{natbib,color}
\usepackage{bm, bbm,epstopdf}
\usepackage{xr, zref, hyperref}

\newenvironment{proof}[1][Proof]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

\newcommand{\qed}{\nobreak \ifvmode \relax \else
      \ifdim\lastskip<1.5em \hskip-\lastskip
      \hskip1.5em plus0em minus0.5em \fi \nobreak
      \vrule height0.75em width0.5em depth0.25em\fi}

%- Makes the section title start with Appendix in the appendix environment
 \newcommand{\Appendix}
 {%\appendix
 \def\thesection{\Alph{section}}
 \def\thesubsection{\Alph{section}.\arabic{subsection}}
 \def\theequation{\Alph{section}.\arabic{equation}}
 \def\thealg{\Alph{section}.\arabic{alg}}
 %\def\thesubsection{A.\arabic{subsection}}
 }

\DeclareMathOperator{\vect}{vec}
\DeclareMathOperator{\vech}{vech}
\DeclareMathOperator{\diag}{diag}

\newtheorem{alg}{Algorithm}

% if variable blind is undefined, assume it is 0
\makeatletter
\@ifundefined{blind}{\def\blind{0}}{}
\makeatother

% if not blinded, reference unblinded appendix
\if0\blind
{
  \externaldocument{laplaceapp}
}\fi

% if blinded, reference blinded appendix
\if1\blind
{
  \externaldocument{laplaceappblind}
}\fi


\begin{document}
\thispagestyle{empty} \baselineskip=28pt

\begin{center}
{\LARGE{\bf Independent Metropolis-Hastings Steps for Generalized Linear Models with Latent Gaussian Processes via Global Conditional Laplace Approximations}}
\end{center}

\baselineskip=12pt
%%
\vskip 2mm
% if not blinded, give the authors
\if0\blind
{
  \begin{center}
    Matthew Simpson\footnote{(\baselineskip=10pt to whom correspondence should be addressed)
      Department of Statistics, University of Missouri,
      146 Middlebush Hall, Columbia, MO 65211-6100, themattsimpson@gmail.com}
    % Matthew Simpson,\footnote{(\baselineskip=10pt to whom correspondence should be addressed)
    % Department of Statistics, University of Missouri,
    % 146 Middlebush Hall, Columbia, MO 65211-6100, themattsimpson@gmail.com}
    % Christopher K. Wikle,\footnote{\label{note:aff}\baselineskip=10pt
    % Department of Statistics, University of Missouri,
    % 146 Middlebush Hall, Columbia, MO 65211-6100}
    % and Scott H. Holan\textsuperscript{\ref{note:aff}}
  \end{center}
} \fi

\vskip 2mm
\begin{center}
{\large{\bf Abstract}}
\end{center}


\baselineskip=12pt

\baselineskip=12pt
\par\vfill\noindent
{\bf KEY WORDS:}

\par\medskip\noindent


\clearpage\pagebreak\newpage \pagenumbering{arabic}
\baselineskip=24pt

\section{Introduction}

\section{Fitting GLMMs with LGPs}\label{sec:glm}
We consider a class of generalized linear mixed models \citep[GLMMs;][]{stroup2012generalized} with latent Gaussian processes (LGPs). We will conceptualize our class of models using the strategy of \citet{berliner1996hierarchical} and \citet{wikle2003hierarchical}, that is hierarchically with a data model conditional on parameters and a latent process, a process model conditional on parameters, and finally a parameter model. Suppose we observe an $m$-dimensional vector $\bm{z}_i$ for each observational unit $i$, and let $\bm{z}_{1:n} = (\bm{z}_1', \bm{z}_2', \dots, \bm{z}_n')'$ where $n$ is the number of observational units. We assume these observations are conditionally independent given a set of observation-specific location parameters and some common data level parameter $\bm{\phi}$. Let $\bm{\mu}_i$ denote the location parameter for observation $i$ and $\bm{\mu}_{1:n} = (\bm{\mu}_1', \bm{\mu}_2, \dots, \bm{\mu}_n')'$. Then the data model is $\bm{z}_i|\bm{\mu}, \bm{\phi} \stackrel{ind}{\sim} [\bm{z}|\bm{\mu}_i,\bm{\phi}]$ where $[\bm{z}|\bm{\mu},\bm{\phi}]$ is some known parameterized family of densities for the $\bm{z}_i$s indexed by $\bm{\mu}$ and $\bm{\phi}$; often an exponential family. The location parameters are a known function of a LGP, i.e. $\bm{\mu}_i = \bm{g}(\bm{y}_i)$ where $\bm{g}$ is known and $\bm{y}_{1:n}$ is a LGP where $\bm{y}_{1:n} = (\bm{y}_1', \bm{y}_2', \dots, \bm{y}_n')'$. Then at the process level we model $\bm{y}_{1:n}$ as sum of function of fixed effects and random effects, namel $\bm{y}_{1:n} = \bm{X}_{1:n}\bm{\beta} + \bm{S}_{1:n}\bm{\delta}$ where $\bm{X}_{1:n}$ and $\bm{S}_{1:n}$ are known $mn\times p$ and $mn\times r$ matrices respectively, i.e. $\bm{X}_{1:n} = (\bm{X}_1', \bm{X}_2', \dots, \bm{X}_n')'$ where $\bm{X}_i$ a $m\times p$ matrix and similarly for $\bm{S}_{1:n}$. In addition $\bm{\beta}$ and $\bm{\delta}$ are unknown $p$-dimensional and $r$-dimensional column vectors respectively. Further, we assume $\bm{\delta} \sim N(\bm{0}, \bm{\Sigma})$ The process model can be more succinctly written as $\bm{y}_{1:n} \sim N(\bm{X}_{1:n}\bm{\beta}, \bm{S}_{1:n}\bm{\Sigma}\bm{S}_{1:n}')$ so that $\bm{y}_{1:n}$s mean is structured by $\bm{X}_{1:n}$ and its covariance matrix is structured by $\bm{S}_{1:n}$. Further, we assume that $\bm{\Sigma}$ depends on the unknown parameter $\bm{\phi}$ to allow for, e.g., parsimonious representations of $\bm{\Sigma}$. We allow $\bm{\phi}$ to enter both the data model and $\bm{\Sigma}$, though in many applications they will depend on distinct components of $\bm{\phi}$. The leaves the unknown parameters $(\bm{\phi}, \bm{\beta})$, which in the parameter model obtain a joint prior distribution denoted by $[\bm{\phi}, \bm{\beta}]$. For notational simplicity, let $\bm{D}_n = (\bm{z}_{1:n}, \bm{X}_{1:n}, \bm{S}_{1:n})$ denote all data in the model and $\bm{\theta} = (\bm{\phi}, \bm{\beta})$ denote all model parameters.

A common MCMC approach in these models is data augmentation \citep[DA;][]{tanner1987calculation}. That is, a two step Gibbs sampler consiting of a draw from $[\bm{\theta}|\bm{y}_{1:n},\bm{D}_n]$ and a draw from $[\bm{y}_{1:n}|\bm{\theta},\bm{D}_n]$ iterated repeatedly. In GLMMs with non-Gaussian data models, $[\bm{y}_{1:n}|\bm{\theta},\bm{D}_n]$ is typically intractable and thus requires a MH step. Often a random walk proposal is used here, but if a good approximation for $[\bm{y}_{1:n}|\bm{\theta}, \bm{D}_n]$ is available, then an independent MH (IMH) Gibbs step is attractive. Laplace approximations (LAs) provide a useful approach for constructing a good IMH proposal density for $\bm{y}_{1:n}$'s conditional posterior.


\section{Laplace Approximations}\label{sec:laplace}
Let $\bm{\omega}$ be the model parameter and $\bm{D}_n$ the observed data so that $[\bm{\omega}|\bm{D}_{n}]$ is the posterior distribution, available up to a normalizing constant. Further let $\bm{\omega}^*_n$ denote the posterior mode of $[\bm{\omega}|\bm{D}_{n}]$ and let $\bm{H}_n(\bm{\omega}^*_n)$ denote the Hessian matrix of $\log [\bm{\omega}|\bm{D}_{n}]$ evaluated at $\bm{\omega}_n^*$. Then under suitable regularity conditions $\bm{\omega}$'s posterior distribution is asymptotically normal \citep[Sections~7.4.2~and~7.4.3]{schervish1997theory}, so for a fixed but large value of $n$, $\bm{\omega}|\bm{D}_{n} \stackrel{a}{\sim} N(\bm{\omega}_n^*, -\bm{H}_n^{-1}(\bm{\omega}^*_n))$ where the notation $\stackrel{a}{\sim}$ means ``approximately distributed as.'' We do not discuss the precise technical conditions necessary for this result here, but intuitively for the approximation to be good each element of $\bm{\omega}$ must have enough ``observations'' for asymptotics to kick in. We put ``observations'' in scare quotes because often rather than data, it is other elements of $\bm{\omega}$ are directly informing on another element, e.g. in the GLMM case above $\bm{\omega} = (\bm{\theta}, \bm{y}_{1:n})$ and $\bm{y}_{1:n}$ is directly informing on $\bm{\theta}$. In that case the asymptotic argument does not apply to $\bm{\omega}$, but it typically does apply for $\bm{\theta}$ alone so that if $\bm{y}_{1:n}$ assumed to be Gaussian conditional on $\bm{\theta}$, then a normal approximation may still be reasonable for the posterior of $\bm{\omega}$. It ``may'' be reasonable because the non-Gaussian data model implies that $\bm{y}_{1:n}$ is non-Gaussian in its full conditional posterior, but it is still often approximately Gaussian. We call this LA a global LA (GLA) since it approximates the full joint posterior of $\bm{\omega}=(\bm{\theta},\bm{y}_{1:n})$. 

If the GLA is good, it can be used as a proposal distribution in a single step IMH algorithm to draw from $\bm{\omega}$'s posterior distribution. Typically a $t_{df}$ proposal is used instead of a normal with $df$ set small enough so that its tails dominate the posterior's, which ensures that the Markov chain is uniformly ergodic \citep[Theorem~7.8]{robert2013monte}. Often $\bm{\theta}$ constains too many parameters relative to the amount of data available so that $\bm{\theta}$ is not approximately normal in its marginal posterior, but because of the latent Gaussian assumption $\bm{y}_{1:n}$ is still approximately normal in it full conditional posterior. In this context, a conditional LA can be constructed to the density $[\bm{y}_{1:n}|\bm{\theta},\bm{D}_n]$ directly. We call this a \emph{local} conditional LA (LCLA) since the LA approximation is recomputed for each value of $\bm{\theta}$. Using the LCLA in the context of a Gibbs sampler as the IMH proposal for $\bm{y}_{1:n}$ while sampling $\bm{theta}$ in one or more hopefully conjugate form Gibbs steps is attractive when the LCLA is good since the acceptance rate of the Metropolis step will be high. However, constructing any of these LAs typically requires numerical optimization since the data model is non-Gaussian. For the GLA this optimization only needs to be performed once, so as long as it is doable it only represents a fixed cost of running the resulting IMH algorithm. However, the IMHwG algorithm that results from the LCLA requires a numerical optimization to be run each MCMC iteration in order to update the conditional posterior mode and approximate covariance matrix. So while IMWwG using the LCLA may have nice MCMC properties, the computational cost can make it significantly less attractive and in some cases even prohibitive.

We propose using a simple method in order to keep the fixed cost associated with the GLA from becoming a variable cost when apply to a conditional distribution: use the conditional distribution implied by the GLA as the IMH proposal. We call this a global \emph{conditional} LA (GCLA) since it is an approximation to $\bm{y}_{1:n}$'s conditional posterior that relies on the GLA. More formally, suppose the GLA is given by the posterior mode $(\bm{\theta}^*, \bm{y}_{1:n}^*)$ with approximate covariance matrix $\bm{\Omega}^*$ given by
\begin{align*}
\bm{\Omega}^* = (-\bm{H}^*)^{-1} = \begin{bmatrix} \bm{\Omega}^*_{\theta} & \bm{\Omega}^*_{\theta y} \\ \bm{\Omega}^*_{y \theta} & \bm{\Omega}^*_{y} \end{bmatrix}.
\end{align*}
Then the GCLA to $\bm{y}_{1:n}$'s conditional posterior is $\bm{y}_{1:n}|\bm{\theta},\bm{D}_n \stackrel{a}{\sim} N(\widetilde{\bm{y}}_{1:n}, \widetilde{\bm{\Omega}}_{y})$, where $\widetilde{\bm{y}}_{1:n} = \bm{y}_{1:n}^* + \bm{\Omega}_{y\theta}^*(\bm{\Omega}_{\theta}^*)^{-1}(\bm{\theta} - \bm{\theta}^*)$ and $\widetilde{\bm{\Omega}}_{y} = \bm{\Omega}_{y}^* - \bm{\Omega}_{y\theta}^*(\bm{\Omega}_{\theta}^*)^{-1}\bm{\Omega}_{\theta y}^*$. The GCLA may be worse than the LCLA since the LCLA directly approximates the target conditional posterior, but the computation savings realized from only having to run the optimization algorithm once are typically significant and make it worthwhile. A problem with using either approximation is that by moving from an IMH algorithm to an IMHwG algorithm the mixing and convergence properties of the chain do deteriorate to the extent that $\bm{\theta}$ and $\bm{y}_{1:n}$ are dependent in the posterior. In practice this can often be solved by reparameterizing the model, e.g. using a non-centered parameterization \citep{gelfand1995efficient,roberts1997updating,van2001art,bernardo2003non}.




\section{Spatially Modeling County Population Estimates}\label{sec:pop}
The American Community Survey (ACS) provides 5-year period estimates of county populations as recently as 2014. In 2014 there were 3,142 counties in the United States, including the District of Columbia and counties in Alaska and Hawaii.  We use two separate data models in order to illustrate when both the joint and global conditional Laplace approximations work well. The first is a Poisson data model, i.e., $Z_i \sim \mathrm{Poisson}(\lambda_i)$ where $\lambda_i = \exp(Y_i)$. Through visual inspection of a histogram, it was determined that log county populations look approximately normally distributed, so our second data model is $\log Z_i \sim N(\mu_i, \phi^2)$, where $\mu_i = Y_i$.

The process model in both cases is a reduced rank spatial model $Y = \bm{X}\bm{\beta} + \bm{S}\bm{\delta}$, where $\bm{X}\bm{\beta}$ represents the process mean at each county and $\bm{S}\bm{\delta}$ implies the spatial correlation across counties. The spatial correlation term consists of a set of $r$ basis functions evaluated at each of the $n=3,142$ counties, denoted by the $n\times r$ matrix $\bm{S}$, and a common random effect $\bm{\delta}$. We assume that $\bm{\delta}$ is $r$-dimensional with $r \ll n$ so that the model is reduced rank. Any set of spatial basis functions could be used for $\bm{S}$ but we use the Moran's I (MI) basis set, described below, but see \citet{hughes2013dimension}, \citet{porter2015bayesian}, \citet{bradley2015multivariate} and references therein for additional discussion. Another possibility is to define a reduced rank model for a point-level spatial process using a basis function expansion and compute the implied set of basis functions for each of the census tracts by integrating the point level basis functions appropriately. See Sections 2.1, 3.1, and 4 of \citet{bradley2016regionalization} for details.

The MI basis functions are defined through the orthogonal projection matrix $\bm{P}_{\bm{X}} = \bm{X}(\bm{X}'\bm{X})^{-1}\bm{X}'$. Let $\bm{A}$ denote the binary adjacency matrix with $a_{ij} = 1$ if counties $i$ and $j$ are neighbors, $a_{ij}=0$ otherwise, and $a_{ii}=0$ along the diagonal, and define the MI operator $\bm{G}$ as
\begin{align*}
\bm{G} = (\bm{I}_n - \bm{P}_{\bm{X}})\bm{A}(\bm{I}_n - \bm{P}_{\bm{X}})
\end{align*}
where $\bm{I}_n$ is the $n\times n$ identity matrix. The spectral decomposition of $\bm{G}$ is $\bm{G} = \bm{\Phi}\bm{\Lambda}\bm{\Phi}'.$ To use a reduced rank version of the MI basis functions we truncate the basis function expansion and take $\bm{S}$ to be the $n\times r$ matrix formed by the $r$ columns of $\bm{\Phi}$ corresponding to the $r$ largest in magnitude eigenvalues of $\bm{G}$. The random effect $\bm{\delta}$ is further modeled as $\bm{\delta} \sim N(\bm{0}_r,\bm{\Sigma}(\bm{\theta}))$ where $\bm{0}_r$ denotes an $r$-dimensional vector of zeroes and $\bm{\Sigma}(\bm{\theta})$ is an unknown covariance matrix that depends on the parameter $\bm{\theta}$. The covariance matrix of $\bm{S}\bm{\delta}$ is then $\bm{S}\bm{\Sigma}(\bm{\theta})\bm{S}'$.

The process model depends on choices for $\bm{X}$ and $r$. In practice $r$ can be chosen using a sensitivity analysis. Since our goal is to illustrate computational methods, we will elide choosing $r$ in a principled way and instead use several values for $r$ in order to illustrate when the parameter space becomes too high dimensional for our method to be advantageous. For simplicity we choose an intercept only model, but all derivations for the model here and in Appendix \ref{app:pop} assume that $\bm{X}$ is $n\times p$.

Finally, we consider two extreme parameterizations of $\bm{\Sigma}(\bm{\theta})$ --- the iid parameterization where $\bm{\Sigma}=\sigma^2\bm{I}_r$ and the full parameterization where $\bm{\Sigma}$ is fully parameterized. In the iid parameterization we assume that $\sigma^2 \sim IG(a_\sigma, b_\sigma)$ in the prior, while in the full parameterization we assume that $\bm{\Sigma}\sim IW(d, \bm{E})$. The prior for $\bm{\beta}$ in all models is $\bm{\beta} \sim N(\bm{b}, v^2\bm{I}_p)$, and in the lognormal models the prior for $\phi^2$ is $\phi^2 \sim IG(a_\phi, b_\phi)$. We assume that the parameters $\bm{\beta}$, $\bm{\Sigma}$, and when applicable $\phi^2$ are mutually independent in the prior. For our examples we assume that $a_\sigma = a_\phi = b_\sigma = b_\phi = 1$, $\bm{b} = \bm{0}_p$, $v = 10$, $d = r + 1$ and $\bm{E} = \bm{I}_p$. Often a more complicated prior is appropriate on variance or covariance matrix parameters so that the marginal posterior is not sensitive to arbitrary choices in the prior \citep{gelman2006prior}. We use these conditionally conjugate priors because they allow for a fair comparison between MCMC algorithms --- most alternatives will complicate alternative Gibbs samplers with extra steps or necessitate Metropolis steps making IMH or IMHwG relatively more attractive.

Between the two possible parameterizations of $\bm{\Sigma}$ and the two choices for the data model we consider four possible classes of models. Both for implementing PSO algorithms to find the Laplace approximations and for running the IMH and IMHwG algorithms, we reparameterize the variance and covariance matrix parameters so that they have support on an unconstrained space. For $\sigma^2$ and $\phi^2$ we use the log transformation, and for $\bm{\Sigma}$ we use the Cholesky factorization of $\bm{\Sigma}^{-1}$ allowing the diagonal elements to be negative. See Appendix \ref{app:pop} for details about the posterior distributions, including this transformation and relevant full conditionals.


\section{Predicting the 1988 presidential election}\label{sec:pres}
\citet[Chapter~14]{gelman2006data} describes a model used to predict state-level opinions about the 1988 presidential candidates from national polls in order to predict the outcome of the election. They model the responses to a series of seven polls conducted by CBS News during the week before the 1988 presidential election. The variable of interest is binary: $Z_i=1$ if the $i$th respondent said they supported the Republican candidate and $Z_i=0$ if they said they supported the Democratic candidate, with undecideds being excluded. Focusing on the last poll, they ultimately estimate a logistic regression model with fixed effects for race (whether the respondent was African American or not), sex, and race$\times$sex, and random effects for four age categories, four education categories, and 16 age$\times$education categories, as well as for the respondent's state of residence (including District of Columbia). The mean of the state random effect distribution is one of five region random effects plus the proportion of the state that voted republican in the last election times a slope coefficient.

The model is somewhat overparameterized since it has age, education, and age$\times$education random effects, so we reduce its size by omitting the age and education random effects. In our model the age$\times$education random effects now represent the random effects for each age$\times$education category rather than an interaction term. The single poll data model is
\begin{align*}
P(Z_i = 1) &= \theta_i,\ \ \   \theta_i = \exp(Y_i)/\{1 + \exp(Y_i)\}, \\
Y_i &= \beta_0 + f_i\beta_f + b_i\beta_b + f_ib_i\beta_{fb} + \alpha_{ae}[ae_i] + \alpha_{s}[s_i]\mbox{\ \ \ \ (single poll data model), }
\end{align*}
where $f_i$ indicates whether respondent $i$ identified as female, $b_i$ indicates whether respondent $i$ identified as African American, $ae_i$ indicates respondent $i$'s age$\times$education category, and $s_i$ indicates respondent $i$'s state of residence. Here we use $\alpha_{s}[k]$ to denote the $k$th element of the vector $\bm{\alpha}_s$, so $\bm{\alpha}_{ae}$ contains 16 elements, and $\bm{\alpha}_s$ contains 51 elements (50 states plus the District of Columbia). The single poll process model is
\begin{align*}
\alpha_s[k] \stackrel{ind}{\sim}& N(\alpha_r[r_k] + prev_k\beta_{prev}, \sigma^2_s) \mbox{ for } k=1,2,\dots,51,\\
\alpha_{ae}[k] \stackrel{iid}{\sim}& N(0, \sigma^2_{ae}) \mbox{ for } k=1,2,\dots,16,\\
\alpha_{r}[k] \stackrel{iid}{\sim}& N(0, \sigma^2_{r}) \mbox{ for } k=1,2,\dots,5 \mbox{\ \ \ \ (single poll process model), }
\end{align*}
where $\alpha_r[r_k]$ denotes the region containing state $k$ and $prev_k$ denotes the average vote share for the Republicans in the previous three presidential elections. This model expands the class of models discussed in Section \ref{sec:glm} by allowing the mean of $\bm{\delta}$ to depend on random effects that are further modeled, but adding a level to the hierarchy does not fundamentally change the applicability of the Laplace approximations.

The last poll had 2,015 respondents, but together all seven polls have 11,566 respondents. Using each poll with a minimal number of additional parameters to account for poll to poll variability should increase the quality of the model and result in a posterior with better Laplace approximations since there is so much more data. We analyze a model for all of the polls using the following data model
\begin{align*}
P(Z_i = 1) &= \theta_i,\ \ \   \theta_i = \exp(Y_i)/\{1 + \exp(Y_i)\}, \\
Y_i &= \beta_0 + f_i\beta_f + b_i\beta_b + f_ib_i\beta_{fb} + \alpha_{ae}[ae_i] + \alpha_{s}[s_i] + \alpha_p[p_i]\mbox{\ \ \ \ (all polls data model), }
\end{align*}
where $p_i$ denotes which poll respondent $i$ was surveyed in. The process model is given by
\begin{align*}
\alpha_s[k] \stackrel{ind}{\sim}& N(\alpha_r[r_k] + prev_k\beta_{prev}, \sigma^2_s) \mbox{ for } k=1,2,\dots,51,\\
\alpha_{ae}[k] \stackrel{iid}{\sim}& N(0, \sigma^2_{ae}) \mbox{ for } k=1,2,\dots,16,\\
\alpha_{r}[k] \stackrel{iid}{\sim}& N(0, \sigma^2_{r}) \mbox{ for } k=1,2,\dots,5,\\
\alpha_{p}[k] \stackrel{iid}{\sim}& N(0, \sigma^2_{p}) \mbox{ for } k=1,2,\dots,7 \mbox{\ \ \ \ (all polls process model). }
\end{align*}
In both models we assume each of the $\beta$s have independent $N(0,1000)$ priors, and each of the
$\sigma^2$s have $IG(1,1)$ priors. Including the random effects the single poll model contains 80 parameters while the all polls model contains 89 parameters. Writing down the log posteriors and deriving the Hessians is straightforward but tedious for these models, so we omit these steps, though note that for both the PSO and IMH algorithms the variances should be transformed to the log scale. For MCMC algorithms with multiple Gibbs steps, in single poll model of Section~\ref{sec:pres}, we draw $(\beta_0, \beta_f, \beta_b, \bm{\alpha}_{s}, \bm{\alpha}_{ae})$ in the Metropolis step, while in the all polls model we draw all of those parameters and additionally $\bm{\alpha}_p$ in the Metropolis step. Then for both models there are two additional Gibbs steps --- one where each random effect variance is drawn, and one where $(\beta_{prev}, \bm{\alpha}_r)$ is drawn. These full conditionals are straightforward to derive, so we do not reproduce them here. [SHOULD WE ADD THESE DETAILS INTO AN APPENDIX?]


\section{Discussion}\label{sec:discuss}

\clearpage\pagebreak\newpage\thispagestyle{empty}
\bibliographystyle{../jasa}
\bibliography{../pso}
\end{document}
