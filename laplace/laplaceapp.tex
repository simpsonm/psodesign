%
\documentclass[12pt]{article}
% Oct 26, 1999
\setlength{\oddsidemargin}{-0.125in}
\setlength{\topmargin}{-0.5in} \setlength{\textwidth}{6.5in}
\setlength{\textheight}{9in}

\setlength{\textheight}{9in} \setlength{\textwidth}{6.5in}
\setlength{\topmargin}{-40pt} \setlength{\oddsidemargin}{0pt}
\setlength{\evensidemargin}{0pt}

\setlength{\textheight}{8.5in} \setlength{\textwidth}{6.5in}
\setlength{\topmargin}{-36pt} \setlength{\oddsidemargin}{0pt}
\setlength{\evensidemargin}{0pt} \tolerance=500
\renewcommand{\baselinestretch}{1.5}

\usepackage{amssymb, amsmath, latexsym, array, morefloats, epsfig, rotating, graphicx}
\usepackage{subfigure, url, mathtools, enumerate, wasysym, threeparttable, lscape}
\usepackage{natbib,color}
\usepackage{bm, bbm,epstopdf}
\usepackage{tikz}
\newcount\mycount
\usepackage{xr, zref, hyperref}

\newenvironment{proof}[1][Proof]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

\newcommand{\qed}{\nobreak \ifvmode \relax \else
      \ifdim\lastskip<1.5em \hskip-\lastskip
      \hskip1.5em plus0em minus0.5em \fi \nobreak
      \vrule height0.75em width0.5em depth0.25em\fi}

%- Makes the section title start with Appendix in the appendix environment
 \newcommand{\Appendix}
 {%\appendix
 \def\thesection{\Alph{section}}
 \def\thesubsection{\Alph{section}.\arabic{subsection}}
 \def\theequation{\Alph{section}.\arabic{equation}}
 \def\thefigure{\Alph{section}.\arabic{figure}}
 \def\thealg{\Alph{section}.\arabic{alg}}
 %\def\thesubsection{A.\arabic{subsection}}
 }

\DeclareMathOperator{\vect}{vec}
\DeclareMathOperator{\vech}{vech}
\DeclareMathOperator{\diag}{diag}

\newtheorem{alg}{Algorithm}

% if variable blind is undefined, assume it is 0
\makeatletter
\@ifundefined{blind}{\def\blind{0}}{}
\makeatother

% if not blinded, reference unblinded paper
\if0\blind
{
  \externaldocument{laplace}
}\fi

% if blinded, reference blinded paper
\if1\blind
{
  \externaldocument{laplaceblind}
}\fi


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\thispagestyle{empty} \baselineskip=28pt

\thispagestyle{empty} \baselineskip=28pt

%
\begin{center}
\begin{center}
{\LARGE{\bf Supplemental Web Material for Independent Metropolis-Hastings Steps for Generalized Linear Models with Latent Gaussian Processes via Global Conditional Laplace Approximations}}
\end{center}
\end{center}


\baselineskip=12pt


%%
%%
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\vskip 2mm
% if not blinded, list the authors on the title page
\if0\blind
{
  \begin{center}
    Matthew Simpson,\footnote{(\baselineskip=10pt to whom correspondence should be addressed)
      Department of Statistics, University of Missouri,
      146 Middlebush Hall, Columbia, MO 65211-6100, themattsimpson@gmail.com}
    % Christopher K. Wikle,\footnote{\label{note:aff}\baselineskip=10pt
    % Department of Statistics, University of Missouri,
    % 146 Middlebush Hall, Columbia, MO 65211-6100}
    % and Scott H. Holan\textsuperscript{\ref{note:aff}}
  \end{center}
} \fi

\clearpage\pagebreak\newpage \pagenumbering{arabic}

\baselineskip=24pt

\Appendix
\section{County Population Model Details}\label{app:pop}
We consider two possible data models, Poisson and lognormal, and two possible random effect distributions, iid normal and normal with a fully correlated covariance matrix. The posterior distributions for the models with iid random effects can be written as
\begin{align*}
p(\bm{\beta},& \sigma^2, \bm{\delta}, \phi^2|\bm{z}, \bm{X}, \bm{S}) \propto (\phi^2)^{-n/2 - a_\phi - 1}\exp\left[-\frac{1}{\phi^2}\left\{\frac{(\log\bm{z} - \bm{y})'(\log\bm{z} - \bm{y})}{2} + b_\phi\right\}\right] \\
&\times (\sigma^2)^{-\frac{r}{2} - a_{\sigma}-1}\exp\left\{-\frac{1}{\sigma^2}\left(\frac{\bm{\delta}'\bm{\delta}}{2} + b_{\sigma}\right)\right\}\exp\left\{-\frac{(\bm{\beta} - \bm{b})'(\bm{\beta} - \bm{b})}{2v}\right\}\mbox{\ \ \ \ \ (iid lognormal),}\\
p(\bm{\beta},& \sigma^2, \bm{\delta}|\bm{z}, \bm{X}, \bm{S}) \propto \prod_{i=1}^n \frac{\exp\{-\exp(y_i)\}\exp(y_iz_i)}{z_i!}  \\
&\times (\sigma^2)^{-\frac{r}{2} - a_{\sigma}-1}\exp\left\{-\frac{1}{\sigma^2}\left(\frac{\bm{\delta}'\bm{\delta}}{2} + b_{\sigma}\right)\right\}\exp\left\{-\frac{(\bm{\beta} - \bm{b})'(\bm{\beta} - \bm{b})}{2v}\right\}\mbox{\ \ \ \ \ (iid Poisson).}
\end{align*}
For fully parameterized $\bm{\Sigma}$ we write the posteriors in terms of the precision matrix $\bm{\Omega} = \bm{\Sigma}^{-1}$, yielding
\begin{align*}
p(\bm{\beta},& \bm{\Omega}, \bm{\delta}, \phi^2|\bm{z}, \bm{X}, \bm{S}) \propto (\phi^2)^{-n/2 - a_\phi - 1}\exp\left\{-\frac{1}{\phi^2}\left(\frac{(\log\bm{z} - \bm{y})'(\log\bm{z} - \bm{y})}{2} + b_\phi\right)\right\} \\
&\times |\bm{\Omega}|^{(d - r)/2}\exp\left\{-\frac{1}{2}\left(\bm{\delta}'\bm{\Omega}\bm{\delta}+\frac{(\bm{\beta} - \bm{b})'(\bm{\beta} - \bm{b})}{v} + \mathrm{tr}(\bm{E}\bm{\Omega})\right)\right\}\mbox{\ \ \ \ \ (full lognormal),}\\
p(\bm{\beta},& \bm{\Omega}, \bm{\delta}|\bm{z}, \bm{X}, \bm{S}) \propto \prod_{i=1}^n \frac{\exp\{-\exp(y_i)\}\exp(y_iz_i)}{z_i!}  \\
&\times |\bm{\Omega}|^{(d - r)/2}\exp\left\{-\frac{1}{2}\left(\bm{\delta}'\bm{\Omega}\bm{\delta}+\frac{(\bm{\beta} - \bm{b})'(\bm{\beta} - \bm{b})}{v} + \mathrm{tr}(\bm{E}\bm{\Omega})\right)\right\}\mbox{\ \ \ \ \ (full Poisson)}
\end{align*}
where $\mathrm{tr}(\cdot)$ denotes the trace operator. For the PSO algorithm and some MCMC algorithms it will be easier to work with the Cholesky decomposition of $\bm{\Omega}$ given by $\bm{L}\bm{L}'=\bm{\Omega}$ where $\bm{L}$ is lower triangular. It is often more convenient to put the prior distribution directly on $\bm{L}$ rather than on $\bm{\Omega}$ or $\bm{\Omega}^{-1}$ and solving for the Jacobian, but this depends in part on which MCMC algorithm is used to fit the model. So while we use the prior distribution on $\bm{L}$ implied by a Wishart prior on $\bm{\Omega}$, in practice it is advantageous to use one of the priors suggested by \citet{chen2003random} or \citet{fruhwirth2008bayesian}. We allow the diagonal entries of $\bm{L}$ to be negative in the PSO and IMH algorithms, so $\bm{L}$ is not strictly speaking a Cholesky decomposition. The determinant of the Jacobian is the same in both cases up to a proportionality constant. The signs of the elements of $\bm{L}$ are not identified, therefore care needs to be taken when interpreting the results of MCMC, but transforming back to the precision matrix in a post processing step is sufficient. Let $\ell_{ij}$ denote the $(i,j)$th element of $\bm{L}$. Then the Jacobian of $\bm{\Omega}\to\bm{L}$ is given by
\begin{align*}
|J(\bm{\Omega}\to\bm{L})| \propto \prod_{k=1}^r |\ell_{kk}|^{r + 1 - k}
\end{align*}
where $\bm{\Omega}$ is $r\times r$. Under this parameterization the full posteriors can be written as
\begin{align*}
p(\bm{\beta}, \bm{L}, \bm{\delta}, \phi^2|\bm{z}, \bm{X}, \bm{S})& \propto (\phi^2)^{-n/2 - a_\phi - 1}\exp\left[-\frac{1}{\phi^2}\left\{\frac{(\log\bm{z} - \bm{y})'(\log\bm{z} - \bm{y})}{2} + b_\phi\right\}\right]\\
&\times  \prod_{k=1}^r (\ell_{kk}^2)^{(d - k + 1)/2}\\
&\times \exp\left[-\frac{1}{2}\left\{\bm{\delta}'\bm{L}\bm{L}'\bm{\delta}+\frac{(\bm{\beta} - \bm{b})'(\bm{\beta} - \bm{b})}{v} + \mathrm{tr}(\bm{E}\bm{L}\bm{L}')\right\}\right]\mbox{\ \ \ \ \ (full lognormal),}\\
p(\bm{\beta}, \bm{L}, \bm{\delta}|\bm{z}, \bm{X}, \bm{S})& \propto \prod_{i=1}^n \frac{\exp\{-\exp(y_i)\}\exp(y_iz_i)}{z_i!} \times \prod_{k=1}^r (\ell_{kk}^2)^{(d - k + 1)/2}  \\
&\times \exp\left[-\frac{1}{2}\left\{\bm{\delta}'\bm{L}\bm{L}'\bm{\delta}+\frac{(\bm{\beta} - \bm{b})'(\bm{\beta} - \bm{b})}{v} + \mathrm{tr}(\bm{E}\bm{L}\bm{L}')\right\}\right]\mbox{\ \ \ \ \ (full Poisson).}
\end{align*}
In Appendix \ref{subapp:hess} we derive the Hessian for the fully parameterized Poisson model. The other models are analogous, though the variances in the iid models and in the lognormal models should be transformed to the log scale first.

\subsection{Full conditionals for MCMC}
We consider sevaeral alternative Gibbs sampling algorithms, detailed in Appendix \ref{app:altmcmc}, which draw the covariance matrix from its full conditional distribution. When the covariance matrix is fully parameterized the full conditional distribution of the precision matrix $\bm{\Omega}$ is Wishart, that is $\bm{\Omega} \sim W(\widetilde{d}, \widetilde{\bm{E}})$. This draw is usually accomplished via the Bartlett decomposition \citep{smith1972algorithm}. Let $\widetilde{\bm{C}}$ be the lower triangular Cholesky decomposition of $\widetilde{\bm{E}}$. Then let $\bm{A}$ be an $r\times r$ random lower triangular matrix with independent elements $\{a_{ij}:0<i\le j\le r\}$ where $a_{ii} \sim \sqrt{\chi^2_{\widetilde{d} - i + 1}}$ for $i=1,2,\dots,r$ and $a_{ij} \sim N(0,1)$ for $0<i\le j\le r$. Then $\bm{\Omega} = \bm{L}\bm{L}' \sim W(\widetilde{d}, \widetilde{\bm{E}})$ where $\bm{L} = \widetilde{\bm{C}}\bm{A}$. In the process of drawing $\bm{\Omega}$ we must first draw $\bm{L}$, so we construct our Gibbs samplers in terms of $\bm{L}$ instead of $\bm{\Omega}$.

IMH applies straightforwardly to each of these models, though see Appendix \ref{subapp:hess} for a detailed derivation of the Hessian for the county population models with a fully parameterized covariance matrix associated. To apply IMHwG in the county population models, we draw $(\bm{\beta},\bm{\delta})$ in the Metropolis step and $\sigma^2$, $\bm{L}$, and/or $\phi^2$ in the conditionally conjugate step, depending on the model. The full conditional distribution of $\sigma^2$ in both iid models is $IG(a_{\sigma} + r/2, b_{\sigma} + \bm{\delta}'\bm{\delta}/2)$. The full conditional distribution of $\bm{\Omega}$ in both fully correlated models is $W\{r + 1, (\bm{E} + \bm{\delta}\bm{\delta}')^{-1}\}$. Then a draw from the full conditional distribution of $\bm{L}$ can be obtained via the Bartlett decomposition as described in the previous paragraph. In the lognormal models the full conditional distribution of $\phi^2$ is $IG\{a_{\phi} + n/2, b_{\phi} + (\log \bm{z} - \bm{X}\bm{\beta} - \bm{S}\bm{\delta})'(\log \bm{z} - \bm{X}\bm{\beta} - \bm{S}\bm{\delta})/2\}$. 

\subsection{Deriving the Hessian}\label{subapp:hess}

The log posterior in the fully parameterized Poisson case can be written as
\begin{align*}
\log p(\bm{\beta}&, \bm{L}, \bm{\delta}|\bm{z}, \bm{X}, \bm{S}) = \mathrm{constant}+ \sum_{i=1}^n\left(y_iz_i - \exp(y_i)\right) + \sum_{k=1}^r \frac{d - k + 1}{2}\log\ell_{kk}^2\\
& - \frac{1}{2}\left\{\bm{\delta}'\bm{L}\bm{L}'\bm{\delta}+\frac{(\bm{\beta} - \bm{b})'(\bm{\beta} - \bm{b})}{v^2} + \mathrm{tr}(\bm{E}\bm{L}\bm{L}')\right\}\\ 
=&\ \mathrm{constant} + \sum_{i=1}^n \left\{z_iy_i - \exp(y_i)\right\} + \frac{1}{2}\bm{R}_r\vech(\log\bm{L}^2)\\
&  -\frac{1}{2}\left[\vech(\bm{L})'\bm{M}_r\left\{\bm{K}_r'(\bm{\delta}\bm{\delta}'\otimes \bm{I}_r)\bm{K}_r + (\bm{I}_r\otimes \bm{E})\right\}\bm{M}_r'\vech(\bm{L})+\frac{(\bm{\beta}-\bm{b})'(\bm{\beta}-\bm{b})}{v^2}\right]
\end{align*}
where $y_i = \bm{x}_i'\bm{\beta} + \bm{s}_i'\bm{\delta}$, $\otimes$ is the Kronecker product, $\bm{I}_r$ is the $r\times r$ identity matrix, and $\bm{R}_r$ is an $r(r+1)/2\times r(r+1)/2$ matrix of zeroes except the $(r + 1)k - k(k+1)/2 + k - r$st diagonal element is equal to $d - k + 1$ for $k=1, 2, \dots, r$, corresponding to locations in $\vech(\bm{L})$ that store the diagonal elements of $\bm{L}$. In $\vech(\log\bm{L}^2)$ the log and power are applied element-wise. The expansions of $\bm{\delta}'\bm{L}\bm{L}'\bm{\delta}$ and $\mathrm{tr}(\bm{E}\bm{L}\bm{L}')$ can be derived from the properties of the trace operator, Kronecker products, and the following identities.
\begin{align*}
\bm{\delta}'\bm{L}\bm{L}'\bm{\delta}& = \vect(\bm{L}'\bm{\delta})'\vect(\bm{L}'\bm{\delta}),\\ 
\vect(\bm{L}'\bm{\delta})& = (\bm{\delta}'\otimes \bm{I}_r)\vect(\bm{L}'),\\
\vect(\bm{L}')& = \bm{K}_r\vect(\bm{L}) = \bm{K}_r\bm{M}_r'\vech(\bm{L}),
\end{align*}
and assuming $\bm{E} = \bm{C}\bm{C}'$, then $\mathrm{tr}(\bm{L'}\bm{C}\bm{C}'\bm{L}) = \vect(\bm{C}'\bm{L})'\vect(\bm{C}'\bm{L})$
where $\vect(\cdot)$ is the vectorization operation which stacks each column of its argument on top of each other into a single column vector, $\vech(\cdot)$ is the half-vectorization operator which is similar but omits elements above the diagonal, $\mathrm{tr}$ is the trace operator, $\bm{K}_r$ is the $r^2\times r^2$ commutation matrix such that for any $r\times r$ matrix $\bm{L}$, $\vect(\bm{L}') = \bm{K}_r\vect(\bm{L})$, $\bm{M}_r$ is the $r^2\times r(r+1)/2$ elimination matrix such that for $\vech(\bm{L}) = \bm{M}_r\vect(\bm{L})$ and if additionally $\bm{L}$ is lower triangular, $\vect(\bm{L}) = \bm{M}_r'\vech(\bm{L})$ \citep{magnus1980elimination,magnus1988linear}. Let $\bm{E}_{ij}$ be an $r\times r$ matrix of zeroes with a one in only its $(i,j)$th position, $\bm{u}_{ij} = \vech(\bm{E}_{ij})$, and $\bm{e}_i$ be a $r$-dimensional column vector of zeroes with a one in the $i$th row. Then $\bm{K}_r$ and $\bm{M}_r$ can be written as
\begin{align*}
\bm{K}_r = \sum_{i=1}^r\sum_{j=1}^r\bm{E}_{ij} \otimes \bm{E}_{ij}' && \mbox{ and } &&\bm{M}_r = \sum_{i\ge j}^r\bm{u}_{ij}\otimes \bm{e}_j'\otimes \bm{e}_i'.
\end{align*}
Now the first derivatives of the log posterior are given by
\begin{align*}
\frac{\partial \log p}{\partial \bm{\beta}} =& \sum_{i=1}^n(z_i - e^{y_i})\bm{x}_i' - \frac{(\bm{\beta} - \bm{b})'}{v^2},\\
\frac{\partial \log p}{\partial \bm{\delta}} =& \sum_{i=1}^n(z_i - e^{y_i})\bm{s}_i' - \bm{\delta}'\bm{L}\bm{L}',\\
\frac{\partial \log p}{\partial \vech(\bm{L})} =& -\vech(\bm{L})'\bm{M}_r\left[\bm{K}_r'(\bm{\delta}\bm{\delta}'\otimes \bm{I}_r)\bm{K}_r + (\bm{I}_r\otimes \bm{E})\right]\bm{M}_r' + \bm{R}_r\vech(1/\bm{L}),
\end{align*}
where in $\vech(1/\bm{L})$ the division is applied element-wise. Next the second derivatives are
\begin{align*}
\frac{\partial^2 \log p}{\partial \bm{\beta} \partial \bm{\beta}'} =& -\sum_{i=1}^ne^{y_i}\bm{x}_i\bm{x}_i' - \frac{1}{v^2}\bm{I}_p\\
\frac{\partial^2 \log p}{\partial \bm{\delta} \partial \bm{\delta}'} =&-\sum_{i=1}^ne^{y_i}\bm{s}_i\bm{s}_i' - \bm{L}\bm{L}'\\
\frac{\partial^2 \log p}{\partial \vech(\bm{L}) \partial \vech(\bm{L})'} =& -\bm{M}_r\left[\bm{K}_r'(\bm{\delta}\bm{\delta}'\otimes \bm{I}_r)\bm{K}_r + (\bm{I}_r\otimes \bm{E})\right]\bm{M}_r' - \bm{R}_r\diag\left(\vech(\bm{L})^{-2}\right)\\
\frac{\partial^2 \log p}{\partial \bm{\beta} \partial \bm{\delta}'} =& -\sum_{i=1}^ne^{y_i}\bm{s}_i\bm{x}_i'\\
\frac{\partial^2 \log p}{\partial \bm{\beta} \partial \vech(\bm{L})'} =&\  \bm{0}_{p\times r(r+1)/2}\\
\frac{\partial^2 \log p}{\partial \bm{\delta} \partial \vech(\bm{L})'} =&-\frac{\partial \bm{\delta}'\bm{L}\bm{L}'}{\partial \vech(\bm{L})'} =  -(\bm{\delta}' \otimes \bm{I}_r)(\bm{I}_{r^2} + \bm{K}_r)(\bm{L}\otimes \bm{I}_r) \bm{M}_r',
\end{align*}
where $\diag(\vech(\bm{L})^{-2})$ is a diagonal matrix with the elements of $\vech(\bm{L})$ raised to the power $-2$ along the diagonal. The last second derivative matrix can be derived using repeated application of the chain rule and from the following facts for any $r\times r$ matrix $\bm{A}$ \citep{magnus2007matrix}:
\begin{align*}
\frac{\partial \vect(\bm{A}\bm{A}')}{\partial \vect(\bm{A})} & = (\bm{I}_{r^2} + \bm{K}_r)(\bm{A}\otimes \bm{I}_r)\\
\frac{\partial \bm{A}\bm{\delta}}{\partial \vect(\bm{A})}& = \bm{\delta}' \otimes \bm{I}_r.
\end{align*}
Then, using the chain rule for lower triangular $\bm{L}$ we have
\begin{align*}
\frac{\partial \bm{\delta}'\bm{L}\bm{L'}}{\partial\vech(\bm{L})'} &= \frac{\partial \bm{\delta}'\bm{L}\bm{L'}}{\partial \bm{L}\bm{L}'\bm{\delta}} \frac{\partial \bm{L}\bm{L}'\bm{\delta}}{\partial \vect(\bm{L}\bm{L}')} \frac{\partial \vect(\bm{L}\bm{L}')}{\partial \vect(\bm{L})}\frac{\partial \vect(\bm{L})}{\partial \vech(\bm{L})}\frac{\partial \vech(\bm{L})}{\partial \vech(\bm{L})'}\\
&= \bm{I}_r \frac{\partial \bm{L}\bm{L}'\bm{\delta}}{\partial \vect(\bm{L}\bm{L}')} \frac{\partial \vect(\bm{L}\bm{L}')}{\partial \vect(\bm{L})} \bm{M}_r'\bm{I}_{r(r+1)/2}\\
&= (\bm{\delta}' \otimes \bm{I}_r)(\bm{I}_{r^2} + \bm{K}_r)(\bm{L}\otimes \bm{I}_r) \bm{M}_r'.
\end{align*}
Finally, the Hessian is
\begin{align*}
\bm{H} = \begin{bmatrix} \frac{\partial^2 \log p}{\partial \bm{\beta} \partial \bm{\beta}'} & \frac{\partial^2 \log p}{\partial \bm{\beta} \partial \bm{\delta} '} & \frac{\partial^2 \log p}{\partial \bm{\beta} \partial \vech(\bm{L}) '}\\
\frac{\partial^2 \log p}{\partial \bm{\delta} \partial \bm{\beta}'}  & \frac{\partial^2 \log p}{ \partial \bm{\delta} \partial \bm{\delta}'} & \frac{\partial^2 \log p}{ \partial \bm{\delta} \partial \vech(\bm{L})'}\\
\frac{\partial^2 \log p}{\partial \vech(\bm{L}) \partial \bm{\beta}'} & \frac{\partial^2 \log p}{ \partial \vech(\bm{L}) \partial \bm{\delta}'} & \frac{\partial^2 \log p}{\partial \vech(\bm{L}) \partial \vech(\bm{L})'} 
\end{bmatrix}.
\end{align*}

\section{Alternative MCMC algorithms}\label{app:altmcmc}
[CURRENTLY THIS SECTION ONLY HAS GENERIC VERSIONS OF THE ALGORITHMS. DETAILS FOR OUR CASES?]
This section describes the alternative MCMC algorithms used for comparisons with PSO assisted Metropolis-Hastings algorithms. In particular, we use two types of adaptive random walk Metropolis within Gibbs algorithms. In the generic problem, suppose we wish to sample from the posterior distribution $p(\bm{\theta}|\bm{y})$ using MCMC methods. Suppose that $\bm{\theta} = (\bm{\theta}_1, \bm{\theta}_2)$ and that the full conditional $p(\bm{\theta}_1|\bm{\theta}_2, \bm{y})$ can be sampled from easily while the full conditional for $\bm{\theta}_2$ may be intractable. Then a single move random walk Metropolis within Gibbs (RWwG) algorithm for this problem is as follows.
\begin{alg}[Single move random walk Metropolis within Gibbs]\label{alg:RWwG}
Given target posterior $p(\bm{\theta}_1,\bm{\theta}_2|\bm{y})$ where $\bm{\theta}_2=(\theta_{21},\theta_{22},\dots,\theta_{2n})$, it is easy to sample from $p(\bm{\theta}_1|\bm{\theta}_2,\bm{y})$, and given that the support of $p(\bm{\theta}_{2}|\bm{\theta}_1,\bm{y})$ is $\Re^n$, iteration $t+1$ is obtained from iteration $t$ via
\begin{enumerate}
\item Draw $\bm{\theta}_1^{(t+1)} \sim p(\bm{\theta}_1|\bm{\theta}_2^{(t)},\bm{y})$
\item For $k=1,2\dots,n$
\begin{itemize}
\item[] Draw $\theta_{2k}^{(prop)} \sim N(\theta_{2k}^{(t)}, \eta_k)$ and form the Metropolis acceptance ratio
\begin{align*}
a_k^{(t)} = \frac{p(\theta_{2k}^{(prop)}|\bm{\theta}_1^{(t+1)},\theta_{21}^{(t+1)},,\dots,\theta_{2k-1}^{(t+1)},\theta_{2k+1}^{(t)},\dots,\theta_{2n}^{(t)},\bm{y})}{p(\theta_{2k}^{(t)}|\bm{\theta}_1^{(t+1)},\theta_{21}^{(t+1)},,\dots,\theta_{2k-1}^{(t+1)},\theta_{2k+1}^{(t)},\dots,\theta_{2n}^{(t)},\bm{y})}.
\end{align*}
Then set $\theta_{2k}^{(t+1)}=\theta_{2k}^{(prop)}$ with probability $r_k^{(t)}=\min(a_k, 1)$ and otherwise set $\theta_{2k}^{(t+1)} = \theta_{2k}^{(t)}$
\end{itemize}
\end{enumerate}
\end{alg}
A major problem with this algorithm is that the $\eta_k$s must be selected so that the algorithm Metropolis steps accept a reasonable amount of time. According to \citet{gelman1996efficient} the optimal acceptance rate in a narrow set of problems is 0.44 for a single dimensional random walk, though this is often used as a guideline for more complex problems. We adaptively tune $\eta_k$ during the burn-in period of the chain in a manner discussed in \citet{andrieu2008tutorial}. The computed Metropolis acceptance probability for $\theta_{2k}$ is denoted by $r_{k}^{(t)} = \min(a_k^{(t)}, 1)$; let $r^*$ denote the target acceptance probability. Then we evolve $\eta_k$ over time via
\begin{align*}
\log\ \eta_{k}^{(t+1)} = \log\ \eta_{k}^{(t)} + \gamma^{(t+1)}(r_k^{(t)} - r^*)
\end{align*}
where $\gamma^{(t)}$ is a fixed sequence decreasing to zero fast enough to enough that the algorithm converges. The intuition here is that if the acceptance rate is too high, we can increase the effective search space by increasing the random walk standard deviation, while if it is too low, we can decrease the effective search space by decreasing the random walk standard deviation. We use $r^*=0.44$. A tuning method such as this implies that $(\bm{\theta}^{(1)}, \bm{\theta}^{(2)},\dots)$ is no longer a Markov chain, and additional conditions are required to ensure convergence to the target posterior distribution. So in practice our RWwG algorithms run Algorithm~\ref{alg:RWwG} with the tuning method described above until convergence and until the $\eta_k$s settle into a rough equilibrium, then we use Algorithm~\ref{alg:RWwG} without tuning but using the last known values of the $\eta_k$s. In practice this is equivalent setting $\gamma^{(t)}=0$ after the burn-in period. During the burn-in we set $\gamma^{(t)}=1$ and we initialize at $\eta_k^{(0)}=1$ for all $k$.

An alternative to RWwG algorithms are block random walk Metropolis within Gibbs algorithms (B-RWwG). Suppose we have an estimate of the covariance structure between the elements of $\bm{\theta}_2$ in the posterior. Then 
\begin{alg}[Block random walk Metropolis within Gibbs]\label{alg:B-RWwG}
Given target posterior $p(\bm{\theta}_1,\bm{\theta}_2|\bm{y})$ where it is easy to sample from $p(\bm{\theta}_1|\bm{\theta}_2,\bm{y})$, the support of $p(\bm{\theta}_{2}|\bm{\theta}_1,\bm{y})$ is $\Re^n$, and given an estimate $\bm{\Sigma}$ of $\mathrm{cov}(\bm{\theta}_2|\bm{y})$, iteration $t+1$ is obtained from iteration $t$ via
\begin{enumerate}
\item Draw $\bm{\theta}_1^{(t+1)} \sim p(\bm{\theta}_1|\bm{\theta}_2^{(t)},\bm{y})$
\item Draw $\bm{\theta}_{2}^{(prop)} \sim N(\bm{\theta}_{2}^{(t)}, \eta\bm{\Sigma})$ and form the Metropolis acceptance ratio
\begin{align*}
a = \frac{p(\bm{\theta}_{2}^{(prop)}|\bm{\theta}_1^{(t+1)},\bm{y})}{p(\bm{\theta}_{2}^{(t)}|\bm{\theta}_1^{(t+1)},\bm{y})}.
\end{align*}
Then set $\bm{\theta}_{2}^{(t+1)}=\bm{\theta}_{2}^{(prop)}$ with probability $\min(a,1)$ and otherwise set $\bm{\theta}_{2}^{(t+1)} = \bm{\theta}_{2}^{(t)}$.
\end{enumerate}
\end{alg}
We make this algorithm adaptive as well by tuning $\eta$ in the same fashion as the $\eta_k$s above. The main difference is that this algorithm must be initialized with a crude estimate of $\bm{\Sigma}$ by running another algorithm, e.g. Algorithm \ref{alg:RWwG}, or Algorithm \ref{alg:B-RWwG} with $\bm{\Sigma}$ chosen to be diagonal and manually tuned.

\clearpage\pagebreak\newpage\thispagestyle{empty}
\bibliographystyle{../jasa}
\bibliography{../pso}
\end{document}

%
%
%
%
%
%
%
%
%
%\paragraph{Acknowledgments} We would like to thank an anonymous
%referee for constructive comments that led to improvement of the
%original manuscript.
%
