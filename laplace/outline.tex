\documentclass[12pt]{article}
\setlength{\oddsidemargin}{-0.125in}
\setlength{\topmargin}{-0.5in} \setlength{\textwidth}{6.5in}
\setlength{\textheight}{9in}

\setlength{\textheight}{9in} \setlength{\textwidth}{6.5in}
\setlength{\topmargin}{-40pt} \setlength{\oddsidemargin}{0pt}
\setlength{\evensidemargin}{0pt}

\setlength{\textheight}{8.5in} \setlength{\textwidth}{6.5in}
\setlength{\topmargin}{-36pt} \setlength{\oddsidemargin}{0pt}
\setlength{\evensidemargin}{0pt} \tolerance=500
\renewcommand{\baselinestretch}{1.5}

\usepackage{amssymb, amsmath, latexsym, array, morefloats, epsfig, rotating, graphicx}
\usepackage{subfigure, url, mathtools, enumerate, wasysym, threeparttable, lscape}
\usepackage{natbib,color}
\usepackage{bm, bbm,epstopdf}
\usepackage{xr, zref, hyperref}

\newenvironment{proof}[1][Proof]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

\newcommand{\qed}{\nobreak \ifvmode \relax \else
      \ifdim\lastskip<1.5em \hskip-\lastskip
      \hskip1.5em plus0em minus0.5em \fi \nobreak
      \vrule height0.75em width0.5em depth0.25em\fi}

%- Makes the section title start with Appendix in the appendix environment
 \newcommand{\Appendix}
 {%\appendix
 \def\thesection{\Alph{section}}
 \def\thesubsection{\Alph{section}.\arabic{subsection}}
 \def\theequation{\Alph{section}.\arabic{equation}}
 \def\thealg{\Alph{section}.\arabic{alg}}
 %\def\thesubsection{A.\arabic{subsection}}
 }

\DeclareMathOperator{\vect}{vec}
\DeclareMathOperator{\vech}{vech}
\DeclareMathOperator{\diag}{diag}

\newtheorem{alg}{Algorithm}

% if variable blind is undefined, assume it is 0
\makeatletter
\@ifundefined{blind}{\def\blind{0}}{}
\makeatother

% if not blinded, reference unblinded appendix
\if0\blind
{
  \externaldocument{psostatapp}
}\fi

% if blinded, reference blinded appendix
\if1\blind
{
  \externaldocument{psostatapp}
}\fi


\begin{document}
\thispagestyle{empty} \baselineskip=28pt

\begin{center}
{\LARGE{\bf OUTLINE FOR Independent Metropolis-Hastings Steps for Generalized Linear Models with Latent Gaussian Processes via Global Conditional Laplace Approximations}}
\end{center}

\baselineskip=12pt
%%
\vskip 2mm
% if not blinded, give the authors
\if0\blind
{
  \begin{center}
    Matthew Simpson\footnote{(\baselineskip=10pt to whom correspondence should be addressed)
      Department of Statistics, University of Missouri,
      146 Middlebush Hall, Columbia, MO 65211-6100, themattsimpson@gmail.com}
    % Matthew Simpson,\footnote{(\baselineskip=10pt to whom correspondence should be addressed)
    % Department of Statistics, University of Missouri,
    % 146 Middlebush Hall, Columbia, MO 65211-6100, themattsimpson@gmail.com}
    % Christopher K. Wikle,\footnote{\label{note:aff}\baselineskip=10pt
    % Department of Statistics, University of Missouri,
    % 146 Middlebush Hall, Columbia, MO 65211-6100}
    % and Scott H. Holan\textsuperscript{\ref{note:aff}}
  \end{center}
} \fi

\vskip 2mm
\begin{center}
{\large{\bf Abstract}}
\end{center}

\baselineskip=12pt

\baselineskip=12pt
\par\vfill\noindent
{\bf KEY WORDS:}

\par\medskip\noindent


\clearpage\pagebreak\newpage \pagenumbering{arabic}
\baselineskip=24pt

\section{Outline}
(Yes I know the title is long)
\subsection{Basic Problem}
Want to sample from a difficult posterior density. Suppose it has model parameter $\theta$ and latent process gaussian process $y$, and we observe data $z$. The target posterior distribution is $[\theta, y|z] \propto [z|y,\theta][y|\theta][\theta]$ where $[y|\theta]$ is a normal density. A common MCMC strategy for this class of models is a data augmentation Gibbs sampler, i.e. draw $\theta \sim [\theta|y,z]$ and $y \sim [y|\theta,z]$ iteratively. Often $[\theta|y,z]$ is a relatively easy to sample from, but $[y|\theta,z]$ is not a density of known form and so requires a Metropolis step.
\subsection{Laplace approximations}
There are three ways we can employ Laplace approximations here:
\begin{enumerate}
\item Global Laplace approximation as a joint Metropolis proposal for $[y,\theta|z]$. But $\theta$ is often non-normal in the posterior.
\item Local conditional Laplace approximation as a proposal for $[y|\theta,z]$, i.e. compute the Laplace approximation to $[y|\theta,z]$ every iteration of MCMC. But then we have to do numerical optimization to find $y$'s conditional mode every iteration of the MCMC, which can be expensive.
\item Global conditional Laplace approximation as a proposal for $[y|\theta,z]$, i.e. compute the global Laplace approximation once, then compute the implied conditional distribution for $[y|\theta,z]$ every iteration. Often much cheaper because much of the computation can be pre-computed. (This is the main contribution)
\end{enumerate}

\subsection{Basic structure of the paper}
\begin{enumerate}
\item Introduction
\item Describe the problem for latent Gaussian process models.
\item Describe Laplace approximations and introduce the ``Global conditional Laplace approximation'' (GCLA)
\item Give some intuition for when the GCLA will be good vs. a GLA or a LCLA (global LA and local conditional LA). Maybe a theorem that explains why/when GCLA works about as well as LCLA. Maybe another theorem that explains when no CLA should work well. (Note: theorems may not be worth the time because it's not obvious to me how to go about proving them right now)
\item 2-4 examples illustrating both good and bad, with some simulations. (It doesn't always work. In particular, when the data model is highly non-normal, it can be extremely poor).
\end{enumerate}

\subsection{PSO tie in?}
It's possible that the best of our PSO algorithms is actually good at finding the posterior mode in some cases. Worth checking, and if so, we have another minor contribution (and a chance to cite the STAT paper we're pushing out).

\section{The Examples}
\subsection{County population model}
This illustrates when the GLA is bad, but the GCLA is good rather nicely.
\subsection{Election model}
This expands the class of models somewhat; not sure if it illustrates anything very nicely. I'll have to think about this one a bit.
\subsection{Unemployment Rates model}
(County or tract or even state level). This was a model we jettisoned awhile ago, but basically it's a GLMM with a LGP where the data model is a Beta distribution. The GCLA doesn't work here because the data model is too non-normal - basically the uncertainties associated with the unemployment rates are too high and the rates are too close to zero, so the Beta distributions are U-shaped. But if we but a sufficiently tight prior on sufficiently high levels of certainty, I think we can get the GCLA to work. So it illustrates when  ANY conditional LA should work well and when it shouldn't.

\clearpage\pagebreak\newpage\thispagestyle{empty}
\bibliographystyle{jasa}
\bibliography{../pso}
\end{document}
