 \documentclass[12pt]{article}
\setlength{\oddsidemargin}{-0.125in}
\setlength{\topmargin}{-0.5in} \setlength{\textwidth}{6.5in}
\setlength{\textheight}{9in}

\setlength{\textheight}{9in} \setlength{\textwidth}{6.5in}
\setlength{\topmargin}{-40pt} \setlength{\oddsidemargin}{0pt}
\setlength{\evensidemargin}{0pt}

\setlength{\textheight}{8.5in} \setlength{\textwidth}{6.5in}
\setlength{\topmargin}{-36pt} \setlength{\oddsidemargin}{0pt}
\setlength{\evensidemargin}{0pt} \tolerance=500
\renewcommand{\baselinestretch}{1.5}

\usepackage{amssymb, amsmath, latexsym, array, morefloats, epsfig, rotating, graphicx}
\usepackage{subfigure, url, mathtools, enumerate, wasysym, threeparttable, lscape}
\usepackage{natbib,color}
\usepackage{bm, bbm,epstopdf}
\usepackage{xr, zref, hyperref}

\newenvironment{proof}[1][Proof]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

\newcommand{\var}{\mathrm{var}}
\newcommand{\cov}{\mathrm{cov}}
\newcommand{\E}{\mathrm{E}}
\newcommand{\tr}{\mathrm{tr}}

\newcommand{\qed}{\nobreak \ifvmode \relax \else
      \ifdim\lastskip<1.5em \hskip-\lastskip
      \hskip1.5em plus0em minus0.5em \fi \nobreak
      \vrule height0.75em width0.5em depth0.25em\fi}

%- Makes the section title start with Appendix in the appendix environment
 \newcommand{\Appendix}
 {%\appendix
 \def\thesection{\Alph{section}}
 \def\thesubsection{\Alph{section}.\arabic{subsection}}
 \def\theequation{\Alph{section}.\arabic{equation}}
 \def\thealg{\Alph{section}.\arabic{alg}}
 %\def\thesubsection{A.\arabic{subsection}}
 }

\DeclareMathOperator{\vect}{vec}
\DeclareMathOperator{\vech}{vech}
\DeclareMathOperator{\diag}{diag}

\newtheorem{alg}{Algorithm}

% if variable blind is undefined, assume it is 0
\makeatletter
\@ifundefined{blind}{\def\blind{0}}{}
\makeatother

% if not blinded, reference unblinded appendix
% \if0\blind
% {
%   \externaldocument{stdesignapp}
% }\fi

% % if blinded, reference blinded appendix
% \if1\blind
% {
%   \externaldocument{stdesignapp}
% }\fi


\begin{document}
\thispagestyle{empty} \baselineskip=28pt

\begin{center}
{\LARGE{\bf Particle Swarm Optimization for Spatial Design}}
\end{center}

\baselineskip=12pt
%%
\vskip 2mm
% if not blinded, give the authors
\if0\blind
{
  \begin{center}
    Matthew Simpson\footnote{(\baselineskip=10pt to whom correspondence should be addressed)
      Department of Statistics, University of Missouri,
      146 Middlebush Hall, Columbia, MO 65211-6100, themattsimpson@gmail.com}
    % Matthew Simpson,\footnote{(\baselineskip=10pt to whom correspondence should be addressed)
    % Department of Statistics, University of Missouri,
    % 146 Middlebush Hall, Columbia, MO 65211-6100, themattsimpson@gmail.com}
    % Christopher K. Wikle,\footnote{\label{note:aff}\baselineskip=10pt
    % Department of Statistics, University of Missouri,
    % 146 Middlebush Hall, Columbia, MO 65211-6100}
    % and Scott H. Holan\textsuperscript{\ref{note:aff}}
  \end{center}
} \fi

\vskip 2mm
\begin{center}
{\large{\bf Abstract}}
\end{center}
\baselineskip=12pt 

\baselineskip=12pt
\par\vfill\noindent
{\bf KEY WORDS:} 

\par\medskip\noindent


\clearpage\pagebreak\newpage \pagenumbering{arabic}
\baselineskip=24pt

\section{The Problem}
Suppose we are interested in the latent spatial field of some response variable $Y(\bm{s})$, $\bm{s}\in \mathcal{D}\subseteq \Re^2$. Specifically, we are interested in predicting $Y(\bm{s})$ at a set of target locations $\bm{s}_1, \bm{s}_2, \dots, \bm{s}_M\in\mathcal{D}$. We have the ability to sample $N$ locations anywhere in $\mathcal{D}$, and we wish to place them in order to optimize some design criterion. Let $\bm{d}_1, \bm{d}_2, \dots, \bm{d}_N\in\mathcal{D}$ denote the $N$ sampled locations. Suppose that $Y(\bm{s})$ is a geostatistical process with mean function $\mu(\bm{s})=\bm{x}(\bm{s})'\bm{\beta}$ for some known covariate $\bm{x}(\bm{s})$ and covariance function $C(\bm{s}, \bm{t})$ for $\bm{s},\bm{t}\in\mathcal{D}$. Once the design points are selected, we observe $Z(\bm{d}_i)$ for $i=1,2,\dots,N$ where $Z(\bm{d}) = Y(\bm{d}) + \varepsilon(\bm{d})$ and $\varepsilon(\bm{d})$ is mean zero white noise with variance $\sigma^2_{\varepsilon}$, representing measurement error. An intuitive criterion to minimize is the average mean square prediction error (MSPE) from kriging across each of the target locations, each the errrors are weighted appropriately. It turns out that this is intimately related to a more principled design criterion, the expected entropy gain on the predictive distribution: $\E\left\{\log[Y(\bm{s}_0)|\bm{Z}] - \log[Y(\bm{s}_0)]\right\}$. Since the prior does not depend on the design, this is equivalent to maximizing $\E\left\{\log[Y(\bm{s}_0)|\bm{Z}]\right\}$.

\subsection{Simple Kriging}
In simple kriging, $C(.,.)$, $\bm{\beta}$, and $\sigma^2_{\varepsilon}$ are all treated as known. Let $\bm{Z} = [Z(\bm{d}_1), Z(\bm{d}_2), \dots, Z(\bm{d}_N) ]'$, $\bm{X} = [\bm{x}(\bm{d}_1), \bm{x}(\bm{d}_2), \dots, \bm{x}(\bm{d}_N)]'$, $\bm{C}_Z = \cov(\bm{Z})$ where $\cov[Z(\bm{d}_i), Z(\bm{d}_j)] = C(\bm{d}_i,\bm{d}_j) + \sigma^2_\varepsilon 1(\bm{d}_i = \bm{d}_j)$, and $\bm{c}_Y(\bm{s}_0) = \cov[Y(\bm{s}_0), \bm{Z}]$ where $\cov[Y(\bm{s}_0), Z(\bm{d}_i)] = C(\bm{s}_0, \bm{d}_i)$. Then the simple kriging predictor of $Y(\bm{s}_0)$ is the linear predictor $\bm{Y}^*(\bm{s}_0) = \bm{\lambda}'\bm{Z} + k$ that MSPE conditional on sampled locations, $\bm{d}_1, \bm{d}_2, \dots, \bm{d}_N$. Specifically it minimizes $\E[Y(\bm{s}_0) - \widehat{Y}_{sk}(\bm{s}_0)]^2$ over $\bm{\lambda}$ and $k$ such that $\bm{\lambda}'\bm{1}=1$ where $\bm{1}$ is a column vector of ones. The simple kriging predictor is easily derived as 
\begin{align*}
\widehat{Y}_{sk}(\bm{s}_0) &= \bm{x}(\bm{s}_0)'\bm{\beta} + \bm{c}_Y(\bm{s}_0)'\bm{C}_Z^{-1}(\bm{Z} - \bm{X}\bm{\beta})\\
\intertext{ with MSPE }
\sigma_{sk}^{2}(\bm{s}_0) &= C(\bm{s}_0,\bm{s}_0) - \bm{c}_Y(\bm{s}_0)'\bm{C}_Z^{-1}\bm{c}_Y(\bm{s}_0).
\end{align*}
The simple kriging MSPE a function of the design points, $\bm{D}=(\bm{d}_1,\bm{d}_2,\dots,\bm{d}_N)$, through $\bm{c}_Y(\bm{s}_0)$ and $\bm{C}_Z^{-1}$. 

From a Bayesian perspective, this predictor can be rationalized by a particular Bayesian hierachical model (HM) that has a bit more structure. Specifically, we now assume that $\{Y(\bm{s}):\bm{s}\in\mathcal{D}\}$ and $\{\varepsilon(\bm{s}):\bm{s}\in\mathcal{D}\}$ are independent Gaussian processes. Then the posterior predictive distribution for $Y(\bm{s}_0)$ can be derived as
\begin{align*}
[Y(\bm{s}_0)|\bm{Z}] &= \int [Y(\bm{s}_0)|\bm{Y}, \bm{Z}][\bm{Y}|\bm{Z}]d\bm{Y}\\
\intertext{where it is easy to derive that}
Y(\bm{s}_0)|\bm{Y},\bm{Z} &\sim N\left[\bm{x}(\bm{s}_0)'\bm{\beta} + \bm{c}_Y(\bm{s}_0)'\bm{C}_Y^{-1}(\bm{Y} - \bm{X}\bm{\beta}), C(\bm{s}_0, \bm{s}_0) - \bm{c}_Y(\bm{s}_0)'\bm{C}_Y^{-1}\bm{c}_Y(\bm{s}_0)\right],\\
\bm{Y}|\bm{Z} &\sim N\left[\bm{X}\bm{\beta} + \bm{C}_Y\bm{C}_Z^{-1}(\bm{Z} - \bm{X}\bm{\beta}), \bm{C}_Y - \bm{C}_Y\bm{C}_Z^{-1}\bm{C}_Y\right]
\intertext{so that $Y(\bm{s}_0)|\bm{Z}$ is Gaussian with}
\E[Y(\bm{s}_0)|\bm{Z}] &= \E\left\{\E[Y(\bm{s}_0)|\bm{Y},\bm{Z}]|\bm{Z}\right\} = \bm{x}(\bm{s}_0)'\bm{\beta} + \bm{c}_Y(\bm{s}_0)'\bm{C}_Z^{-1}(\bm{Z} - \bm{X}\bm{\beta}),\\
\var[Y(\bm{s}_0)|\bm{Z}]&=\E\left\{\var[Y(\bm{s}_0)|\bm{Y},\bm{Z}]|\bm{Z}\right\} + \var\left\{\E[Y(\bm{s}_0)|\bm{Y},\bm{Z}]|\bm{Z}\right\}\\
&= C(\bm{s}_0, \bm{s}_0) - \bm{c}_Y(\bm{s}_0)'\bm{C}_Z^{-1}\bm{c}_Y(\bm{s}_0).
\end{align*}
Then a design criterion we could minimize is average simple kriging MSPE across all target locations, $\sum_{i=1}^N\sigma_{sk}^2(\bm{s}_i)/N$. This is a bit naive, however, because it does not take into account the covariance structure across the locations. 

Let $\bm{Y}^*=[Y(\bm{s}_1,Y(\bm{s}_2,\dots,Y(\bm{s}_M)]'$ denote the true vector of response variables at the target locations, $\{\bm{s}_k\in\mathcal{D}:i=1,\dots,M\}$. The covariance between the prediction errors at $\bm{s}_i$ and $\bm{s}_j$ can be derived as $\cov[Y(\bm{s}_i),Y(\bm{s}_j)|\bm{Z}] = C(\bm{s}_i,\bm{s}_j) - \bm{c}_Y(\bm{s}_i)'\bm{C}_Z^{-1}\bm{c}_Y(\bm{s}_j)$. Let $\bm{C}_{Y*} = \cov(\bm{Y}^*)$ where $\cov[Y(\bm{s}_i),Y(\bm{s}_j)] = C(\bm{s}_i,\bm{s}_j)$ and $\bm{C}_{YY^*}=\cov(\bm{Y},\bm{Y}^*)$ where the $i,j$th element is given by $\cov[Y(\bm{d}_i),Y(\bm{s}_j)] = C(\bm{d}_i,\bm{s}_j)$ for $i=1,2,\dots,N$ and $j=1,2,\dots,M$. Then the simple kriging error covariance matrix is given by $\bm{\Sigma}_{sk} = \bm{C}_{Y^*} - \bm{C}_{YY^*}'\bm{C}_Z^{-1}\bm{C}_{YY^*}$, which corresponds exactly to the posterior predictive covariance matrix for $\bm{Y}^*$.

An intuitive design criterion to minimize is $\log|\bm{\Sigma}_{sk}|$, which is in some sense the total amount of variation in the distribution of the errors from predicting $\bm{Y}^*$. Let $\bm{X}^* = [\bm{x}(\bm{s}_1), \bm{x}(\bm{s}_2), \dots, \bm{x}(\bm{s}_M)]'$. Then if we return to the Bayesian setting $\bm{Y}^*|\bm{Z}$ is Gaussian with mean $\widehat{\bm{Y}}_{sk} = [\widehat{Y}_{sk}(\bm{s}_1), \widehat{Y}_{sk}(\bm{s}_2), \dots, \widehat{Y}_{sk}(\bm{s}_M)]'$ and covariance matrix $\bm{\Sigma}_{sk}$. So if we instead maximize the expected entropy gain, we maximize
\begin{align*}
\E\{\log[\bm{Y}^*|\bm{Z}]\} &= -\frac{M}{2}\log(2\pi) - \frac{1}{2}\log|\bm{\Sigma}_{sk}| - \frac{1}{2}\E\left\{\E[(\bm{Y}^* - \widehat{\bm{Y}}_{sk})'\bm{\Sigma}_{sk}^{-1}(\bm{Y}^* - \widehat{\bm{Y}}_{sk})|\bm{Z}]\right\}\\
&= -\frac{M}{2}\log(2\pi e) - \frac{1}{2}\log|\bm{\Sigma}_{sk}|.
\end{align*}
Thus maximizing expected entropy gain in the distribution of $\bm{Y}^*$ is equivalent to minimizing $\log|\bm{\Sigma}_{sk}|$, our intuitive design criterion.

\subsection{Universal Kriging}
A major limitation of simple kriging is that typically $\bm{\beta}$, $\sigma^2_{\varepsilon}$, and $C(.,.)$ are unknown. Universal kriging attempts to remedy this by allowing $\bm{\beta}$ to be unknown. The universal kriging predictor is
\begin{align*}
\widehat{Y}_{uk}(\bm{s}_0) &= \bm{x}(\bm{s}_0)'\widehat{\bm{\beta}}_{gls} + \bm{c}_Y(\bm{s}_0)'\bm{C}_Z^{-1}(\bm{Z} - \bm{X}\widehat{\bm{\beta}}_{gls})\\
\intertext{where $\widehat{\bm{\beta}}_{gls} = (\bm{X}'\bm{C}_Z^{-1}\bm{X})^{-1}\bm{X}'\bm{C}_Z^{-1}\bm{Z}$ is the generalized least squares estimate of $\bm{\beta}$, and the MSPE of $\widehat{Y}_{uk}(\bm{s}_0)$ is}
\sigma_{uk}^2(\bm{s}_0) &= C(\bm{s}_0, \bm{s}_0) - \bm{c}_Y(\bm{s}_0)'\bm{C}_Z^{-1}\bm{c}_Y(\bm{s}_0)  \\
& + [\bm{x}(\bm{s}_0)  - \bm{X}'\bm{C}_Z^{-1}\bm{c}_Y(\bm{s}_0)]'[\bm{X}'\bm{C}_Z^{-1}\bm{X}][\bm{x}(\bm{s}_0)  - \bm{X}'\bm{C}_Z^{-1}\bm{c}_Y(\bm{s}_0)].
\end{align*}
This can also be justified via a Bayesian HM by using the same Gaussian assumptions as before, but further assuming that $\bm{\beta}$ has the improper uniform prior, i.e. $\bm{\beta} \sim U(-\infty, \infty)$. We will take a slightly more general approach as assume that $\bm{\beta}\sim N(\bm{b}, \bm{S})$ where the improper uniform prior results from $\bm{S} = s^2\bm{I}$ as $s\to\infty$. Let $\bm{C}_Y = \cov(\bm{Y})$ where $\cov(Y(\bm{d}_i),Y(\bm{d}_j)) = C(\bm{d}_i,\bm{d}_j)$. Now we have
\begin{align*}
[Y(\bm{s}_0)|\bm{Z}] &= \int \int [Y(\bm{s}_0)|\bm{Y}, \bm{\beta}, \bm{Z}][\bm{Y}|\bm{Z}]d\bm{Y}d\bm{\beta}\\
\intertext{where}
Y(\bm{s}_0)|\bm{Y},\bm{\beta}, \bm{Z} &\sim N\left[\bm{x}(\bm{s}_0)'\bm{\beta} + \bm{c}_Y(\bm{s}_0)'\bm{C}_Y^{-1}(\bm{Y} - \bm{X}\bm{\beta}), C(\bm{s}_0, \bm{s}_0) - \bm{c}_Y(\bm{s}_0)'\bm{C}_Y^{-1}\bm{c}_Y(\bm{s}_0)\right],\\
\bm{Y}|\bm{\beta},\bm{Z} &\sim N\left[\bm{X}\bm{\beta} + \bm{C}_Y\bm{C}_Z^{-1}(\bm{Z} - \bm{X}\bm{\beta}), \bm{C}_Y - \bm{C}_Y\bm{C}_Z^{-1}\bm{C}_Y\right],\\
\bm{\beta}|\bm{Z} &\sim N\left[\widehat{\bm{\beta}}_{bgls}, (\bm{X}'\bm{C}_Z^{-1}\bm{X} + \bm{S}^{-1})^{-1}\right],
\end{align*}
where $\widehat{\bm{\beta}}_{bgls} =(\bm{X}'\bm{C}_Z^{-1}\bm{X} + \bm{S}^{-1})^{-1}(\bm{X}'\bm{C}_Z^{-1}\bm{Z} + \bm{S}^{-1}\bm{b})$ can be thought of as the Bayesian generalized least squares estimator. Then $Y(\bm{s}_0)|\bm{Z}$ is again Gaussian with
\begin{align*}
\E[&Y(\bm{s}_0)|\bm{Z}] = \E\left\{\E[Y(\bm{s}_0)|\bm{Y},\bm{\beta}, \bm{Z}]|\bm{Z}\right\} = \bm{x}(\bm{s}_0)'\widehat{\bm{\beta}}_{bgls} + \bm{c}_Y(\bm{s}_0)'\bm{C}_Z^{-1}(\bm{Z} - \bm{X}\widehat{\bm{\beta}}_{bgls}),\\
\var[&Y(\bm{s}_0)|\bm{Z}]=\E\left\{\var[Y(\bm{s}_0)|\bm{Y},\bm{\beta},\bm{Z}]|\bm{Z}\right\} + \var\left\{\E[Y(\bm{s}_0)|\bm{Y},\bm{\beta},\bm{Z}]|\bm{Z}\right\}\\
&= C(\bm{s}_0, \bm{s}_0) - \bm{c}_Y(\bm{s}_0)'\bm{C}_Y^{-1}\bm{c}_Y(\bm{s}_0) + \var\left\{[\bm{x}(\bm{s}_0) - \bm{X}'\bm{C}_Y^{-1}\bm{c}_Y(\bm{s}_0)]'\bm{\beta} + \bm{c}_Y(\bm{s}_0)'\bm{C}_Y^{-1}\bm{Y}|\bm{Z}\right\}.
\end{align*}
To compute the last variance term, note that
\begin{align*}
\cov(\bm{Y}|\bm{Z}) &=\E[\cov(\bm{Y}|\bm{\beta},\bm{Z})|\bm{Z}] + \cov[\E(\bm{Y}|\bm{\beta},\bm{Z})|\bm{Z}] \\
& = \bm{C}_Y - \bm{C}_Y\bm{C}_Z^{-1}\bm{C}_Y + (\bm{I} - \bm{C}_Y\bm{C}_Z^{-1})\bm{X}(\bm{X}'\bm{C}_Z^{-1}\bm{X} + \bm{S}^{-1})^{-1}\bm{X}'(\bm{I} - \bm{C}_Z^{-1}\bm{C}_Y)
\intertext{and}
\cov(\bm{Y},\bm{\beta}|\bm{X}) &=\E[\cov(\bm{Y},\bm{\beta}|\bm{\beta},\bm{Z})|\bm{Z}] + \cov[\E(\bm{Y}|\bm{\beta},\bm{Z}), \bm{\beta}|\bm{Z}] \\
& = (\bm{I} - \bm{C}_Y\bm{C}_Z^{-1})\bm{X}(\bm{X}'\bm{C}_Z^{-1}\bm{X} + \bm{S}^{-1})^{-1}.
\end{align*}
Then we have
\begin{align*}
\var&\left[Y(\bm{s}_0)|\bm{Z}\right] =\\
&C(\bm{s}_0, \bm{s}_0) - \bm{c}_Y(\bm{s}_0)'\bm{C}_Y^{-1}\bm{c}_Y(\bm{s}_0)  \\
&+ [\bm{x}(\bm{s}_0) - \bm{X}'\bm{C}_Y^{-1}\bm{c}_Y(\bm{s}_0)]'[\bm{X}'\bm{C}_Z^{-1}\bm{X} + \bm{S}^{-1}]^{-1}[\bm{x}(\bm{s}_0) - \bm{X}'\bm{C}_Y^{-1}\bm{c}_Y(\bm{s}_0)]\\
&+ \bm{c}_Y(\bm{s}_0)'\left\{\bm{C}_Y^{-1} - \bm{C}_Z^{-1} + (\bm{C}_Y^{-1} - \bm{C}_Z^{-1})\bm{X}(\bm{X}'\bm{C}_Z^{-1}\bm{X} + \bm{S}^{-1})^{-1}\bm{X}'(\bm{C}_Y^{-1} - \bm{C}_Z^{-1})\right\}\bm{c}_Y(\bm{s}_0)\\
&+ \bm{c}_Y(\bm{s}_0)'(\bm{C}_Y^{-1} - \bm{C}_Z^{-1})\bm{X}(\bm{X}'\bm{C}_Z^{-1}\bm{X} + \bm{S}^{-1})^{-1}[\bm{x}(\bm{s}_0) - \bm{X}'\bm{C}_Y^{-1}\bm{c}_Y(\bm{s}_0)]\\
&+[\bm{x}(\bm{s}_0) - \bm{X}'\bm{C}_Y^{-1}\bm{c}_Y(\bm{s}_0)]'(\bm{X}'\bm{C}_Z^{-1}\bm{X} + \bm{S}^{-1})^{-1}\bm{x}(\bm{s}_0)'(\bm{C}_Y^{-1} - \bm{C}_Z^{-1})\bm{c}_Y(\bm{s}_0),
\end{align*}
which reduces to
\begin{align*}
\var&\left[Y(\bm{s}_0)|\bm{Z}\right] =C(\bm{s}_0, \bm{s}_0) - \bm{c}_Y(\bm{s}_0)'\bm{C}_Z^{-1}\bm{c}_Y(\bm{s}_0) \\
&+ [\bm{x}(\bm{s}_0) - \bm{X}'\bm{C}_Z^{-1}\bm{c}_Y(\bm{s}_0)]'[\bm{X}'\bm{C}_Z^{-1}\bm{X} + \bm{S}^{-1}]^{-1}[\bm{x}(\bm{s}_0) - \bm{X}'\bm{C}_Z^{-1}\bm{c}_Y(\bm{s}_0)].
\end{align*}
If $\bm{S}=s\bm{I}$, then as $s\to\infty$, $\bm{S}^{-1}\to 0\times \bm{I}$, so $\bm{S}^{-1}$ drops out of all of these equations and $\widehat{\bm{\beta}}_{bgls}\to\widehat{\bm{\beta}}_{gls}$, so that the posterior predictive mean is the same as the universal kriging predictor and the posterior predictive variance is the same as the variance of the universal kriging prediction error.

The covariance between the kriging prediction errors at two locations is
\begin{align*}
\cov&[\widehat{Y}_{uk}(\bm{s}_i),\widehat{Y}_{uk}(\bm{s}_j)] = C(\bm{s}_i,\bm{s}_j) - \bm{c}_Y(\bm{s}_i)'\bm{C}_Z^{-1}\bm{c}_Y(\bm{s}_j)\\
&+ [\bm{x}(\bm{s}_i) - \bm{X}'\bm{C}_Z^{-1}\bm{c}_Y(\bm{s}_i)]'[\bm{X}'\bm{C}_Z^{-1}\bm{X}]^{-1}[\bm{x}(\bm{s}_j) - \bm{X}'\bm{C}_Z^{-1}\bm{c}_Y(\bm{s}_j)]
\end{align*}
and similarly, the posterior predictive covariance between two locations can be derived as
\begin{align*}
\cov&[Y(\bm{s}_i),Y(\bm{s}_j)|\bm{Z}] = C(\bm{s}_i,\bm{s}_j) - \bm{c}_Y(\bm{s}_i)'\bm{C}_Z^{-1}\bm{c}_Y(\bm{s}_j)\\
&+ [\bm{x}(\bm{s}_i) - \bm{X}'\bm{C}_Z^{-1}\bm{c}_Y(\bm{s}_i)]'[\bm{X}'\bm{C}_Z^{-1}\bm{X} + \bm{S}^{-1}]^{-1}[\bm{x}(\bm{s}_j) - \bm{X}'\bm{C}_Z^{-1}\bm{c}_Y(\bm{s}_j)]
\end{align*}
and once again the latter goes to the former as $s\to\infty$ when $\bm{S}=s\bm{I}$. So $\bm{Y}^*$ is Gaussian in the posterior predictive distribution with mean and covariance matrix
\begin{align*}
\widehat{\bm{Y}}_{buk} &= \E[\bm{Y}^*|\bm{Z}] = \bm{X}^{*}\widehat{\bm{\beta}}_{bgls} + \bm{C}_{YY^*}'\bm{C}_Z^{-1}(\bm{Z} - \bm{X}\widehat{\bm{\beta}}_{bgls})\\
\bm{\Sigma}_{buk} &= \bm{C}_{Y^*} - \bm{C}_{YY^*}'\bm{C}_Z^{-1}\bm{C}_{YY*}\\
&+ (\bm{X}^* - \bm{C}_{YY*}'\bm{C}_Z^{-1}\bm{X})(\bm{X}'\bm{C}_Z^{-1}\bm{X} + \bm{S}^{-1})^{-1}(\bm{X}^* - \bm{C}_{YY*}'\bm{C}_Z^{-1}\bm{X})'.
\end{align*}

Minimizing $\log|\bm{\Sigma}_{buk}|$ in order to choose the sampled locations in this model seems intuitive, much like in the simple kriging case. Again this corresponds to maximizing the expected entropy gain. The part of expected entropy gain that depends on the design is
\begin{align*}
\E\left\{\log[\bm{Y}^*|\bm{Z}]\right\} &= -\frac{M}{2} - \frac{1}{2}\log|\bm{\Sigma}_{uk}| - \frac{1}{2}\E\left\{\E[(\bm{Y}^* - \widehat{\bm{Y}}_{uk})'\bm{\Sigma}_{sk}^{-1}(\bm{Y}^* - \widehat{\bm{Y}}_{uk})|\bm{Z}]\right\}\\
&= -\frac{M}{2}\log(2\pi e) - \frac{1}{2}\log|\bm{\Sigma}_{buk}|,
\end{align*}
and maximizing this is equivalent to minimizing $\log|\bm{\Sigma}_{buk}|$. Since $\bm{\Sigma}_{buk} = \bm{\Sigma}_{uk}$ when $\bm{\beta}\sim U(-\infty,\infty)$, this includes the case of minimizing the log determinant of the covariance of the universal kriging prediction errors.
\subsection{Empirical Kriging}
In both simple and universal kriging the predictors and their MSPEs are the same as the posterior predictive means and posterior predictive variances for a suitably chosen Bayesian HM, and in both cases if we wish to choose the sampled locations in order to minimize the log determinant of the covariance matrix of the kriging prediction errors, the problem is equivalent to maximizing the expected entropy gain for the predictive distribution at the target locations. We can easily the entropy method to the case where both $\sigma^2_{\varepsilon}$ and $C(.,.)$ are unknown, called empirical kriging, though it may no longer correspond to the optimal linear predictor. In the non-Bayesian case things are a bit more delicate.

Suppose the covariance function is parameterized by $\bm{\theta}\in\Theta$, denoted by $C_{\bm{\theta}}(.,.)$, and let $\bm{\phi} = (\bm{\theta}, \sigma^2_{\varepsilon})$. Let $\mathcal{I}(\bm{\phi})$ denote the Fisher information matrix for estimating $\bm{\phi}$, e.g. from maximum likelihood and $\widehat{\bm{\phi}}$ the corresponding estimate. Then the empirical kriging estimate of $Y(\bm{s}_0)$ is given by $\widehat{Y}_{ek}(\bm{s}_0) = \widehat{Y}_{uk}(\bm{s}_0;\widehat{\bm{\phi}})$, where we now make the dependence of $\widehat{Y}_{uk}$ on $\bm{\phi}$ explicit. The MSPE of $\widehat{Y}_{ek}$ can be approximated by $\sigma^2_{ek}(\bm{s}_0;\bm{\phi}) \approx \sigma^2_{uk}(\bm{s}_0;\bm{\phi}) + \tr[A(\bm{s}_0;\bm{\phi})\mathcal{I}^{-1}(\bm{\phi})]$ where $A(\bm{s}_0;\bm{\phi}) = \var[\partial \hat{Y}_{uk}(\bm{s}_0;\bm{\phi})/\partial \bm{\phi}]$. [THIS WILL BE HARD TO DERIVE.... DO WE EVEN WANT TO?]


In the Bayesian case, assume that a priori $(\bm{\beta},\bm{\phi})\sim [\bm{\beta},\bm{\phi}]$ --- for now we allow $\bm{\beta}$ to have an arbitrary prior distribution. The part of the expected entropy gain for the predictive distribution that depends on the design is
\begin{align*}
\E\left\{\log[\bm{Y}^*|\bm{Z}]\right\} =& \int \int \log[\bm{Y}^*|\bm{Z}] [\bm{Y}^*,\bm{Z}]d\bm{Y}^*d\bm{Z}\\ 
=& \int\int\log[\bm{Y}^*|\bm{Z}]\int[\bm{Y}^*,\bm{Z}|\bm{\beta},\bm{\phi}][\bm{\beta},\bm{\phi}]d\bm{Y}^*d\bm{Z}d\bm{\beta}d\bm{\phi}.
\end{align*}
This quantity is not available in closed form, so we estimate it via a hybrid of Monte Carlo simulation and marginal likelihood estimation. 

Typically, $[\bm{\beta},\bm{\phi}]$ is easy to simulate from, and $[\bm{Y}^*,\bm{Z}|\bm{\beta},\bm{\phi}]$ is Gaussian, so it is easy to obtain $L$ draws from $[\bm{Y}^*,\bm{Z}]$, denoted by $[(\bm{Y}^*)^{(l)}, \bm{Z}^{(l)}]$ for $l=1,2,\dots,L$. Next, we need an estimator of $\log[\bm{Y}^*|\bm{Z}]$. We can rewrite the density as $[\bm{Y}^*|\bm{Z}] = [\bm{Y}^*,\bm{Z}]/[\bm{Z}]$. Both the numerator and demoninator are marginal likelihoods, and there are numerous ways to estimate these in the literature. In the typical Bayesian design case, we are trying to maximizes the expected entropy gain of the model parameter, i.e. of $[\bm{\beta},\bm{\phi}|\bm{Z}]$. In that case the is the joint distribution $[\bm{Z},\bm{\beta},\bm{\phi}]$, and is available in closed form. Since we are interested in the entropy of the predictive distribution at a fixed set of points, this becomes more complicated.

When $[\bm{\beta},\bm{\phi}]=[\bm{\beta}][\bm{\phi}]$ with $\bm{\beta}\sim N(\bm{b},\bm{S})$, the marginal likelihood estimation problems become easier. Then we have $[\bm{Y}^*|\bm{Z},\bm{\phi}]$ and $[\bm{Z}|\bm{\phi}]$ in closed form from the universal kriging section, and  
\begin{align*}
[\bm{Y}^*,\bm{Z}] = \int[\bm{Y}^*|\bm{Z},\bm{\phi}][\bm{Z}|\bm{\phi}][\bm{\phi}]d\bm{\phi} &&\mbox{and}&&[\bm{Z}] = \int[\bm{Z}|\bm{\phi}][\bm{\phi}]d\bm{\phi}.
\end{align*}
These integrals are still marginal likelihoods, but the dimension of $\bm{\phi}$ is often small enough that numerical integration may be feasible.


\clearpage\pagebreak\newpage\thispagestyle{empty}
%\bibliographystyle{jasa}
\end{document}
