\documentclass[12pt]{article}
\setlength{\oddsidemargin}{-0.125in}
\setlength{\topmargin}{-0.5in} \setlength{\textwidth}{6.5in}
\setlength{\textheight}{9in}

\setlength{\textheight}{9in} \setlength{\textwidth}{6.5in}
\setlength{\topmargin}{-40pt} \setlength{\oddsidemargin}{0pt}
\setlength{\evensidemargin}{0pt}

\setlength{\textheight}{8.5in} \setlength{\textwidth}{6.5in}
\setlength{\topmargin}{-36pt} \setlength{\oddsidemargin}{0pt}
\setlength{\evensidemargin}{0pt} \tolerance=500
\renewcommand{\baselinestretch}{1.5}

\usepackage{amssymb, amsmath, latexsym, array, morefloats, epsfig, rotating, graphicx}
\usepackage{subfigure, url, mathtools, enumerate, wasysym, threeparttable, lscape}
\usepackage{natbib,color}
\usepackage{bm, bbm,epstopdf}
\usepackage{xr, zref, hyperref}

\newenvironment{proof}[1][Proof]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

\newcommand{\qed}{\nobreak \ifvmode \relax \else
      \ifdim\lastskip<1.5em \hskip-\lastskip
      \hskip1.5em plus0em minus0.5em \fi \nobreak
      \vrule height0.75em width0.5em depth0.25em\fi}

%- Makes the section title start with Appendix in the appendix environment
 \newcommand{\Appendix}
 {%\appendix
 \def\thesection{\Alph{section}}
 \def\thesubsection{\Alph{section}.\arabic{subsection}}
 \def\theequation{\Alph{section}.\arabic{equation}}
 \def\thealg{\Alph{section}.\arabic{alg}}
 %\def\thesubsection{A.\arabic{subsection}}
 }

\DeclareMathOperator{\vect}{vec}
\DeclareMathOperator{\vech}{vech}
\DeclareMathOperator{\diag}{diag}

\newtheorem{alg}{Algorithm}

% if variable blind is undefined, assume it is 0
\makeatletter
\@ifundefined{blind}{\def\blind{0}}{}
\makeatother

% if not blinded, reference unblinded appendix
% \if0\blind
% {
%   \externaldocument{stdesignapp}
% }\fi

% % if blinded, reference blinded appendix
% \if1\blind
% {
%   \externaldocument{stdesignapp}
% }\fi


\begin{document}
\thispagestyle{empty} \baselineskip=28pt

\begin{center}
{\LARGE{\bf Particle Swarm Optimization for Spatial Design}}
\end{center}

\baselineskip=12pt
%%
\vskip 2mm
% if not blinded, give the authors
\if0\blind
{
  \begin{center}
    Matthew Simpson\footnote{(\baselineskip=10pt to whom correspondence should be addressed)
      Department of Statistics, University of Missouri,
      146 Middlebush Hall, Columbia, MO 65211-6100, themattsimpson@gmail.com}
    % Matthew Simpson,\footnote{(\baselineskip=10pt to whom correspondence should be addressed)
    % Department of Statistics, University of Missouri,
    % 146 Middlebush Hall, Columbia, MO 65211-6100, themattsimpson@gmail.com}
    % Christopher K. Wikle,\footnote{\label{note:aff}\baselineskip=10pt
    % Department of Statistics, University of Missouri,
    % 146 Middlebush Hall, Columbia, MO 65211-6100}
    % and Scott H. Holan\textsuperscript{\ref{note:aff}}
  \end{center}
} \fi

\vskip 2mm
\begin{center}
{\large{\bf Abstract}}
\end{center}
\baselineskip=12pt 

\baselineskip=12pt
\par\vfill\noindent
{\bf KEY WORDS:} 

\par\medskip\noindent


\clearpage\pagebreak\newpage \pagenumbering{arabic}
\baselineskip=24pt

\section{The Problem}
Suppose we are interested in the latent spatial field of some response variable $Y(\bm{s})$, $\bm{s}\in \mathcal{D}\subseteq \Re^2$. Specifically, we are interested in predicting $Y(\bm{s})$ at a set of target locations $\bm{s}_1, \bm{s}_2, \dots, \bm{s}_M\in\mathcal{D}$. We have the ability to sample $N$ locations anywhere in $\mathcal{D}$, and we wish to place them in order to optimize some design criterion. Let $\bm{d}_1, \bm{d}_2, \dots, \bm{d}_N\in\mathcal{D}$ denote the $N$ sampled locations. Suppose that $Y(\bm{s})$ is a geostatistical process with mean function $\mu(\bm{s})=\bm{x}(\bm{s})'\bm{\beta}$ for some known covariate $\bm{x}(\bm{s})$ and covariance function $C(\bm{s}, \bm{t})$ for $\bm{s},\bm{t}\in\mathcal{D}$. Once the design points are selected, we observe $Z(\bm{d}_i)$ for $i=1,2,\dots,N$ where $Z(\bm{d}) = Y(\bm{d}) + \varepsilon(\bm{d})$ and $\varepsilon(\bm{d})$ is mean zero white noise with variance $\sigma^2_{\varepsilon}$, representing measurement error. An intuitive criterion to minimize is average mean square prediction error (MSPE) from kriging across each of the target locations. 

\subsection{Simple Kriging}
In simple kriging, $C(.,.)$, $\bm{\beta}$, and $\sigma^2_{\varepsilon}$ are all treated as known. Let $\bm{Z} = (Z(\bm{d}_1), Z(\bm{d}_2), \dots, Z(\bm{d}_N) )'$, $\bm{X} = (\bm{x}(\bm{d}_1), \bm{x}(\bm{d}_2), \dots, \bm{x}(\bm{d}_N))'$, $\bm{C}_Z = \mathrm{cov}(\bm{Z})$ where $\mathrm{cov}(Z(\bm{d}_i), Z(\bm{d}_j)) = C(\bm{d}_i,\bm{d}_j) + \sigma^2_\varepsilon 1(\bm{d}_i = \bm{d}_j)$, and $\bm{c}_Y(\bm{s}_0) = \mathrm{cov}(Y(\bm{s}_0), \bm{Z})$ where $\mathrm{cov}(Y(\bm{s}_0), Z(\bm{d}_i)) = C(\bm{s}_0, \bm{d}_i)$. Then the simple kriging predictor of $Y(\bm{s}_0)$ is the linear predictor $\bm{Y}^*(\bm{s}_0) = \bm{\lambda}'\bm{Z} + k$ that MSPE conditional on sampled locations, $\bm{d}_1, \bm{d}_2, \dots, \bm{d}_N$. Specifically it minimizes $\mathrm{E}[Y(\bm{s}_0) - Y^*(\bm{s}_0)]^2$ over $\bm{\lambda}$ and $k$ such that $\bm{\lambda}'\bm{1}=1$ where $\bm{1}$ is a column vector of ones. The simple kriging predictor is easily derived as 
\begin{align*}
Y^*(\bm{s}_0) &= \bm{x}(\bm{s}_0)'\bm{\beta} + \bm{c}_Y(\bm{s}_0)'\bm{C}_Z^{-1}(\bm{Z} - \bm{X}\bm{\beta})\\
\intertext{ with MSPE }
S_{sk}^{2}(\bm{s}_0) &= C(\bm{s}_0,\bm{s}_0) - \bm{c}_Y(\bm{s}_0)'\bm{C}_Z^{-1}\bm{c}_Y(\bm{s}_0).
\end{align*}
The simple kriging MSPE a function of the design points, $\bm{D}=(\bm{d}_1,\bm{d}_2,\dots,\bm{d}_N)$, through $\bm{c}_Y(\bm{s}_0)$ and $\bm{C}_Z^{-1}$. 

From a Bayesian perspective, this predictor can be rationalized by a particular Bayesian hierachical model (HM) that has a bit more structure. Specifically, we now assume that $\{Y(\bm{s}):\bm{s}\in\mathcal{D}\}$ and $\{\varepsilon(\bm{s}):\bm{s}\in\mathcal{D}\}$ are independent Gaussian processes. Then the posterior predictive distribution for $Y(\bm{s}_0)$ can be derived as
\begin{align*}
[Y(\bm{s}_0)|\bm{Z}] &= \int [Y(\bm{s}_0)|\bm{Y}, \bm{Z}][\bm{Y}|\bm{Z}]d\bm{Y}\\
\intertext{where it is easy to derive that}
Y(\bm{s}_0)|\bm{Y},\bm{Z} &\sim N\left[\bm{x}(\bm{s}_0)'\bm{\beta} + \bm{c}_Y(\bm{s}_0)'\bm{C}_Y^{-1}(\bm{Y} - \bm{X}\bm{\beta}), C(\bm{s}_0, \bm{s}_0) - \bm{c}_Y(\bm{s}_0)'\bm{C}_Y^{-1}\bm{c}_Y(\bm{s}_0)\right],\\
\bm{Y}|\bm{Z} &\sim N\left[\bm{X}\bm{\beta} + \bm{C}_Y\bm{C}_Z^{-1}(\bm{Z} - \bm{X}\bm{\beta}), \bm{C}_Y - \bm{C}_Y\bm{C}_Z^{-1}\bm{C}_Y\right]
\intertext{so that $Y(\bm{s}_0)|\bm{Z}$ is Gaussian with}
\mathrm{E}[Y(\bm{s}_0)|\bm{Z}] &= \mathrm{E}\left\{\mathrm{E}[Y(\bm{s}_0)|\bm{Y},\bm{Z}]|\bm{Z}\right\} = \bm{x}(\bm{s}_0)'\bm{\beta} + \bm{c}_Y(\bm{s}_0)'\bm{C}_Z^{-1}(\bm{Z} - \bm{X}\bm{\beta}),\\
\mathrm{var}[Y(\bm{s}_0)|\bm{Z}]&=\mathrm{E}\left\{\mathrm{var}[Y(\bm{s}_0)|\bm{Y},\bm{Z}]|\bm{Z}\right\} + \mathrm{var}\left\{\mathrm{E}[Y(\bm{s}_0)|\bm{Y},\bm{Z}]|\bm{Z}\right\}\\
&= C(\bm{s}_0, \bm{s}_0) - \bm{c}_Y(\bm{s}_0)'\bm{C}_Z^{-1}\bm{c}_Y(\bm{s}_0).
\end{align*}
Then the design criterion we minimize is average simple kriging variance, $\overline{S}^2_{sk} = \sum_{i=1}^NS_{sk}^2(\bm{s}_i)/N$. Note that in order to $\overline{S}^2_{sk}$, we need to know $\bm{x}(\bm{s})$ a priori for each location $\bm{s}\in\mathcal{D}$. Often for this to be feasible, $\bm{x}(\bm{s})$ must be some know function of $\bm{s}$ and not a covariate that must be measured at location $\bm{s}$. For example if $\bm{s}=(u,v)$, then $\bm{x}(\bm{s}) = (1, u, v)'$ requires no additional measurement.

Naively, it appears strange to minimize a {\it posterior} predictive variance prior to seeing the data. However, in the simple kriging setup, it turns out that $\mathrm{var}[Y(\bm{s}_0)|\bm{Z}]$ does not depend on $\bm{Z}$. More generally, a Bayesian would minimize the {\it expected} posterior predictive variance, $\mathrm{E}\left\{\mathrm{var}[Y(\bm{s}_0)|\bm{Z}]\right\}$. 
\subsection{Universal Kriging}
A major limitation of simple kriging is that typically $\bm{\beta}$, $\sigma^2_{\varepsilon}$, and $C(.,.)$ are unknown. Universal kriging attempts to remedy this by allowing $\bm{\beta}$ to be unknown. The universal kriging predictor is
\begin{align*}
\widehat{Y}(\bm{s}_0) &= \bm{x}(\bm{s}_0)'\widehat{\bm{\beta}}_{gls} + \bm{c}_Y(\bm{s}_0)'\bm{C}_Z^{-1}(\bm{Z} - \bm{X}\widehat{\bm{\beta}}_{gls})\\
\intertext{where $\widehat{\bm{\beta}}_{gls} = (\bm{X}'\bm{C}_Z^{-1}\bm{X})^{-1}\bm{X}'\bm{C}_Z^{-1}\bm{Z}$ is the generalized least squares estimate of $\bm{\beta}$, and the MSPE of $\widehat{Y}(\bm{s}_0)$ is}
\overline{S}_{uk}^2(\bm{s}_0) &= C(\bm{s}_0, \bm{s}_0) - \bm{c}_Y(\bm{s}_0)'\bm{C}_Z^{-1}\bm{c}_Y(\bm{s}_0)  \\
& + [\bm{x}(\bm{s}_0)  - \bm{X}'\bm{C}_Z^{-1}\bm{c}_Y(\bm{s}_0)]'[\bm{X}'\bm{C}_Z^{-1}\bm{X}][\bm{x}(\bm{s}_0)  - \bm{X}'\bm{C}_Z^{-1}\bm{c}_Y(\bm{s}_0)].
\end{align*}
This can also be justified via a Bayesian HM by using the same Gaussian assumptions as before, but further assuming that $\bm{\beta}$ has the improper uniform prior, i.e. $\bm{\beta} \sim U(-\infty, \infty)$. Let $\bm{C}_Y = \mathrm{cov}(\bm{Y})$ where $\mathrm{cov}(Y(\bm{d}_i),Y(\bm{d}_j)) = C(\bm{d}_i,\bm{d}_j)$. Now we have
\begin{align*}
[Y(\bm{s}_0)|\bm{Z}] &= \int \int [Y(\bm{s}_0)|\bm{Y}, \bm{\beta}, \bm{Z}][\bm{Y}|\bm{Z}]d\bm{Y}d\bm{\beta}\\
\intertext{where}
Y(\bm{s}_0)|\bm{Y},\bm{\beta}, \bm{Z} &\sim N\left[\bm{x}(\bm{s}_0)'\bm{\beta} + \bm{c}_Y(\bm{s}_0)'\bm{C}_Y^{-1}(\bm{Y} - \bm{X}\bm{\beta}), C(\bm{s}_0, \bm{s}_0) - \bm{c}_Y(\bm{s}_0)'\bm{C}_Y^{-1}\bm{c}_Y(\bm{s}_0)\right],\\
\bm{Y}|\bm{\beta},\bm{Z} &\sim N\left[\bm{X}\bm{\beta} + \bm{C}_Y\bm{C}_Z^{-1}(\bm{Z} - \bm{X}\bm{\beta}), \bm{C}_Y - \bm{C}_Y\bm{C}_Z^{-1}\bm{C}_Y\right],\\
\bm{\beta}|\bm{Z} &\sim N\left[\widehat{\bm{\beta}}_{gls}, (\bm{X}'\bm{C}_Z^{-1}\bm{X})^{-1}\right],
\end{align*}
so that $Y(\bm{s}_0)|\bm{Z}$ is again Gaussian with
\begin{align*}
\mathrm{E}[&Y(\bm{s}_0)|\bm{Z}] = \mathrm{E}\left\{\mathrm{E}[Y(\bm{s}_0)|\bm{Y},\bm{\beta}, \bm{Z}]|\bm{Z}\right\} = \bm{x}(\bm{s}_0)'\widehat{\bm{\beta}}_{gls} + \bm{c}_Y(\bm{s}_0)'\bm{C}_Z^{-1}(\bm{Z} - \bm{X}\widehat{\bm{\beta}}_{gls}),\\
\mathrm{var}[&Y(\bm{s}_0)|\bm{Z}]=\mathrm{E}\left\{\mathrm{var}[Y(\bm{s}_0)|\bm{Y},\bm{\beta},\bm{Z}]|\bm{Z}\right\} + \mathrm{var}\left\{\mathrm{E}[Y(\bm{s}_0)|\bm{Y},\bm{\beta},\bm{Z}]|\bm{Z}\right\}\\
&= C(\bm{s}_0, \bm{s}_0) - \bm{c}_Y(\bm{s}_0)'\bm{C}_Y^{-1}\bm{c}_Y(\bm{s}_0) + \mathrm{var}\left\{[\bm{x}(\bm{s}_0) - \bm{X}'\bm{C}_Y^{-1}\bm{c}_Y(\bm{s}_0)]'\bm{\beta} + \bm{c}_Y(\bm{s}_0)'\bm{C}_Y^{-1}\bm{Y}|\bm{Z}\right\}.
\end{align*}
To compute the last variance term, note that
\begin{align*}
\mathrm{cov}(\bm{Y}|\bm{Z}) &=\mathrm{E}[\mathrm{cov}(\bm{Y}|\bm{\beta},\bm{Z})|\bm{Z}] + \mathrm{cov}[\mathrm{E}(\bm{Y}|\bm{\beta},\bm{Z})|\bm{Z}] \\
& = \bm{C}_Y - \bm{C}_Y\bm{C}_Z^{-1}\bm{C}_Y + (\bm{I} - \bm{C}_Y\bm{C}_Z^{-1})\bm{X}(\bm{X}'\bm{C}_Z^{-1}\bm{X})^{-1}\bm{X}'(\bm{I} - \bm{C}_Z^{-1}\bm{C}_Y)
\intertext{and}
\mathrm{cov}(\bm{Y},\bm{\beta}|\bm{X}) &=\mathrm{E}[\mathrm{cov}(\bm{Y},\bm{\beta}|\bm{\beta},\bm{Z})|\bm{Z}] + \mathrm{cov}[\mathrm{E}(\bm{Y}|\bm{\beta},\bm{Z}), \bm{\beta}|\bm{Z}] \\
& = (\bm{I} - \bm{C}_Y\bm{C}_Z^{-1})\bm{X}(\bm{X}'\bm{C}_Z^{-1}\bm{X})^{-1}.
\end{align*}
Then we have
\begin{align*}
\mathrm{var}&\left[Y(\bm{s}_0)|\bm{Z}\right] =\\
&C(\bm{s}_0, \bm{s}_0) - \bm{c}_Y(\bm{s}_0)'\bm{C}_Y^{-1}\bm{c}_Y(\bm{s}_0)  \\
&+ [\bm{x}(\bm{s}_0) - \bm{X}'\bm{C}_Y^{-1}\bm{c}_Y(\bm{s}_0)]'[\bm{X}'\bm{C}_Z^{-1}\bm{X}][\bm{x}(\bm{s}_0) - \bm{X}'\bm{C}_Y^{-1}\bm{c}_Y(\bm{s}_0)]\\
&+ \bm{c}_Y(\bm{s}_0)'\left\{\bm{C}_Y^{-1} - \bm{C}_Z^{-1} + (\bm{C}_Y^{-1} - \bm{C}_Z^{-1})\bm{X}(\bm{X}'\bm{C}_Z^{-1}\bm{X})^{-1}\bm{X}'(\bm{C}_Y^{-1} - \bm{C}_Z^{-1})\right\}\bm{c}_Y(\bm{s}_0)\\
&+ \bm{c}_Y(\bm{s}_0)'(\bm{C}_Y^{-1} - \bm{C}_Z^{-1})\bm{X}(\bm{X}'\bm{C}_Z^{-1}\bm{X})^{-1}[\bm{x}(\bm{s}_0) - \bm{X}'\bm{C}_Y^{-1}\bm{c}_Y(\bm{s}_0)]\\
&+[\bm{x}(\bm{s}_0) - \bm{X}'\bm{C}_Y^{-1}\bm{c}_Y(\bm{s}_0)]'(\bm{X}'\bm{C}_Z^{-1}\bm{X})^{-1}\bm{x}(\bm{s}_0)'(\bm{C}_Y^{-1} - \bm{C}_Z^{-1})\bm{c}_Y(\bm{s}_0),
\end{align*}
which reduces to
\begin{align*}
\mathrm{var}&\left[Y(\bm{s}_0)|\bm{Z}\right] =C(\bm{s}_0, \bm{s}_0) - \bm{c}_Y(\bm{s}_0)'\bm{C}_Z^{-1}\bm{c}_Y(\bm{s}_0) \\
&+ [\bm{x}(\bm{s}_0) - \bm{X}'\bm{C}_Z^{-1}\bm{c}_Y(\bm{s}_0)]'[\bm{X}'\bm{C}_Z^{-1}\bm{X}][\bm{x}(\bm{s}_0) - \bm{X}'\bm{C}_Z^{-1}\bm{c}_Y(\bm{s}_0)].
\end{align*}
Then the design criterion in this case is $\overline{S}_{uk}^2 = \sum_{i=1}^NS_{uk}^2(\bm{s}_i)/N$, and once again $\mathrm{E}\left\{\mathrm{var}[Y(\bm{s}_0)|\bm{Z}]\right\} = \mathrm{var}[Y(\bm{s}_0)|\bm{Z}]$, which is constant in $\bm{Z}$.

\subsection{Full Uncertainty Kriging}
In both simple and universal kriging the predictors and their MSPEs are the same as the posterior predictive means and posterior predictive variances for a suitably chosen Bayesian HM. In both cases if we wish to choose the sampled locations, $\bm{d}_1,\bm{d}_2,\dots,\bm{d}_N$, in order to minimize the MSPE from the kriging estimate, we are effectively minimizing $\sum_{i=1}^N\mathrm{E}\left\{\mathrm{var}[Y(\bm{s}_i)|\bm{Z}(\bm{D})]\right\}/N$ in $\bm{D}$. We can easily extend this to the case where both $\sigma^2_{\varepsilon}$ and $C(.,.)$ are unknown, though it may no longer correspond to the optimal linear predictor. Suppose the covariance function is parameterized by $\bm{\theta}\in\Theta$, denoted by $C_{\bm{\theta}}(.,.)$, and let $\bm{\phi} = (\bm{\beta}, \bm{\theta}, \sigma^2_{\varepsilon})$. Assume that a priori $\bm{\phi}\sim [\bm{\phi}]$ --- we relax the restriction that $\bm{\beta}\sim U(-\infty, \infty)$. Then we can write the expected posterior predictive variance of $Y(\bm{s}_0)$ as
\begin{align*}
\mathrm{E}\left\{\mathrm{var}[Y(\bm{s}_0)|\bm{Z}]\right\} =& \mathrm{E}\bigg(\mathrm{E}\left\{\mathrm{var}[Y(\bm{s}_0)|\bm{\theta},\sigma^2_{\varepsilon},\bm{Z}]|\bm{Z}\right\}\bigg) + \mathrm{E}\bigg(\mathrm{var}\left\{\mathrm{E}[Y(\bm{s}_0)|\bm{\theta},\sigma^2_{\varepsilon},\bm{Z}]|\bm{Z}\right\}\bigg)\\
\intertext{where from the universal kriging case we know}
\mathrm{E}[Y(\bm{s}_0)|\bm{\theta},\sigma^2_{\varepsilon},\bm{Z}] =&\left\{\left[\bm{x}(\bm{s}_0) - \bm{X}'\bm{C}_Z^{-1}\bm{c}_Y(\bm{s}_0)\right](\bm{X}'\bm{C}_Z^{-1}\bm{X})^{-1}\bm{X}' + \bm{c}_Y(\bm{s}_0)'\right\}\bm{C}_Z^{-1}\bm{Z},\\
\mathrm{var}[Y(\bm{s}_0)|\bm{\theta},\sigma^2_{\varepsilon},\bm{Z}] =&C(\bm{s}_0, \bm{s}_0) - \bm{c}_Y(\bm{s}_0)'\bm{C}_Z^{-1}\bm{c}_Y(\bm{s}_0) \\
&+ [\bm{x}(\bm{s}_0) - \bm{X}'\bm{C}_Z^{-1}\bm{c}_Y(\bm{s}_0)]'[\bm{X}'\bm{C}_Z^{-1}\bm{X}][\bm{x}(\bm{s}_0) - \bm{X}'\bm{C}_Z^{-1}\bm{c}_Y(\bm{s}_0)].
\end{align*}
[PROBLEM: $\mathrm{var}\left\{\mathrm{E}[Y(\bm{s}_0)|\bm{\theta},\sigma^2_{\varepsilon},\bm{Z}]|\bm{Z}\right\}$ IS NOT AVAILABLE IN CLOSED FORM, SO MAYBE WE DO JUST MINIMIZE THE A PRIORI EXPECTED UNIVERSAL KRIGING VARIANCE?]


\clearpage\pagebreak\newpage\thispagestyle{empty}
%\bibliographystyle{jasa}
\end{document}
