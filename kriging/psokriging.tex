 \documentclass[12pt]{article}
\setlength{\oddsidemargin}{-0.125in}
\setlength{\topmargin}{-0.5in} \setlength{\textwidth}{6.5in}
\setlength{\textheight}{9in}

\setlength{\textheight}{9in} \setlength{\textwidth}{6.5in}
\setlength{\topmargin}{-40pt} \setlength{\oddsidemargin}{0pt}
\setlength{\evensidemargin}{0pt}

\setlength{\textheight}{8.5in} \setlength{\textwidth}{6.5in}
\setlength{\topmargin}{-36pt} \setlength{\oddsidemargin}{0pt}
\setlength{\evensidemargin}{0pt} \tolerance=500
\renewcommand{\baselinestretch}{1.5}

\usepackage{amssymb, amsmath, latexsym, array, morefloats, epsfig, rotating, graphicx}
\usepackage{subfigure, url, mathtools, enumerate, wasysym, threeparttable, lscape}
\usepackage{natbib,color}
\usepackage{bm, bbm,epstopdf}
\usepackage{xr, zref, hyperref}

\newenvironment{proof}[1][Proof]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

\newcommand{\var}{\mathrm{var}}
\newcommand{\cov}{\mathrm{cov}}
\newcommand{\E}{\mathrm{E}}
\newcommand{\tr}{\mathrm{tr}}

\newcommand{\qed}{\nobreak \ifvmode \relax \else
      \ifdim\lastskip<1.5em \hskip-\lastskip
      \hskip1.5em plus0em minus0.5em \fi \nobreak
      \vrule height0.75em width0.5em depth0.25em\fi}

%- Makes the section title start with Appendix in the appendix environment
 \newcommand{\Appendix}
 {%\appendix
 \def\thesection{\Alph{section}}
 \def\thesubsection{\Alph{section}.\arabic{subsection}}
 \def\theequation{\Alph{section}.\arabic{equation}}
 \def\thealg{\Alph{section}.\arabic{alg}}
 %\def\thesubsection{A.\arabic{subsection}}
 }

\DeclareMathOperator{\vect}{vec}
\DeclareMathOperator{\vech}{vech}
\DeclareMathOperator{\diag}{diag}

\newtheorem{alg}{Algorithm}

% if variable blind is undefined, assume it is 0
\makeatletter
\@ifundefined{blind}{\def\blind{0}}{}
\makeatother

% if not blinded, reference unblinded appendix
% \if0\blind
% {
%   \externaldocument{stdesignapp}
% }\fi

% % if blinded, reference blinded appendix
% \if1\blind
% {
%   \externaldocument{stdesignapp}
% }\fi


\begin{document}
\thispagestyle{empty} \baselineskip=28pt

\begin{center}
{\LARGE{\bf Particle Swarm Optimization for Spatial Design}}
\end{center}

\baselineskip=12pt
%%
\vskip 2mm
% if not blinded, give the authors
\if0\blind
{
  \begin{center}
    Matthew Simpson\footnote{(\baselineskip=10pt to whom correspondence should be addressed)
      Department of Statistics, University of Missouri,
      146 Middlebush Hall, Columbia, MO 65211-6100, themattsimpson@gmail.com}
    % Matthew Simpson,\footnote{(\baselineskip=10pt to whom correspondence should be addressed)
    % Department of Statistics, University of Missouri,
    % 146 Middlebush Hall, Columbia, MO 65211-6100, themattsimpson@gmail.com}
    % Christopher K. Wikle,\footnote{\label{note:aff}\baselineskip=10pt
    % Department of Statistics, University of Missouri,
    % 146 Middlebush Hall, Columbia, MO 65211-6100}
    % and Scott H. Holan\textsuperscript{\ref{note:aff}}
  \end{center}
} \fi

\vskip 2mm
\begin{center}
{\large{\bf Abstract}}
\end{center}
\baselineskip=12pt 

\baselineskip=12pt
\par\vfill\noindent
{\bf KEY WORDS:} 

\par\medskip\noindent


\clearpage\pagebreak\newpage \pagenumbering{arabic}
\baselineskip=24pt

\section{Particle swarm optimization}\label{sec:pso}
We briefly describe PSO here; refer to \citet{blum2008swarm} for an excellent introduction and \citet{clerc2010particle} for more detail. Suppose that we wish to maximize some objective function $Q(\bm{\theta}):\Re^D\to\Re$. Let $i=1,2,\dots,n$ index a set of particles over time, $t=1,2,\dots,T$, where in every period each particle consists of a location $\bm{\theta}_i(t)\in \Re^D$, a velocity $\bm{v}_i(t) \in \Re^D$, a personal best location $\bm{p}_i(t)\in\Re^D$, and a group best location $\bm{g}_i(t)\in\Re^D$. Here we mean ``best'' in the sense of maximizing $Q$, so $Q(\bm{p}_i(t)) \geq Q(\bm{\theta}_i(s))$ for any $s\leq t$. The group best location is defined with respect to some neighborhood $\mathcal{N}_i$ of particle $i$; that is, $\bm{g}_i(t) = \arg\max_{\{\bm{p}_j(t)|j\in\mathcal{N}_i\}}Q(\bm{p}_j(t))$. In the simplest case where the entire swarm is the neighborhood of each particle, $\bm{g}_i(t)\equiv \bm{g}(t) = \arg\max_{\{\bm{p}_j(t)|j\in 1:n\}}Q(\bm{p}_j(t))$. The generic PSO algorithm updates as follows for each particle $i$:
\begin{align}\label{eq:pso}
\bm{v}_i(t+1) &= \omega \bm{v}_i(t) + \phi_1 \bm{r}_{1i}(t)\circ\{\bm{p}_i(t) - \bm{\theta}_i(t)\} + \phi_2 \bm{r}_{2i}(t)\circ\{\bm{g}_i(t) - \bm{\theta}_i(t)\},\nonumber\\
\bm{\theta}_i(t+1) &= \bm{\theta}_i(t) + \bm{v}_i(t+1),\nonumber\\
\bm{p}_i(t+1) &= \begin{cases} \bm{p}_i(t)   & \mbox{if }\  Q(\bm{p}_i(t)) \ge Q(\bm{\theta}_i(t + 1))\\
                               \bm{\theta}_i(t+1) & \mbox{otherwise},
\end{cases}\nonumber\\
\bm{g}_i(t+1) &= \arg\max_{\{\bm{p}_j(t+1)|j\in\mathcal{N}_i\}}Q(\bm{p}_j(t+1)),
\end{align}
where $\circ$ denotes the Hadamard product (element-wise product), $\bm{r}_{1i}(t)$ and $\bm{r}_{2i}(t)$ are each vectors of $D$ random variates independently generated from the $U(0,1)$ distribution, and $\omega>0$, $\phi_1>0$, and $\phi_2>0$ are user-defined parameters. The term $\omega \bm{v}_i(t)$ controls the particle's tendency to keep moving in the direction it is already going, so $\omega$ is called the inertia parameter. For $\omega<1$ velocities tend to decrease over time, while for $\omega>1$ they tend to increase over time. Similarly $\phi_1 \bm{r}_{1i}(t)\circ\{\bm{p}_i(t) - \bm{\theta}_i(t)\}$ controls the particle's tendency to move towards its personal best location while $\phi_2 \bm{r}_{2i}(t)\circ\{\bm{g}_i(t) - \bm{\theta}_i(t)\}$ controls its tendency to move toward the group's best location, so $\phi_1$ and $\phi_2$ are called the cognitive correction factor and social correction factor, respectively \citep{blum2008swarm}. This version of PSO is equivalent to \citet{clerc2002particle}'s constriction type I particle swarm. There are many variants of the PSO algorithm, often obtained through choosing $(\omega,\phi_1,\phi_2)$ in special ways or sometimes even dynamically. A default version of the algorithm sets $\omega = 0.7298$ and $\phi_1 = \phi_2 = 1.496$; see \citet{clerc2002particle} and \citet{blum2008swarm} for justification of these choices. Even when $\omega<1$, if $\phi_1$ and $\phi_2$ are set high enough the velocities of the particles can continually increase and cause the swarm to make jumps that are much too large. A heavy handed way to solve this problem is by setting an upper bound on the velocity of any particle in any given direction, called velocity clamping. However, the default parameter values suggested by \citet{clerc2002particle} are also designed to prevent exactly this sort of velocity explosion.

Any PSO variant can also be combined with various neighborhood topologies that control how the particles communicate to each other. The default global topology allows each particle to see each other particle's previous best location for the social components of their respective velocity updates, but this can cause inadequate exploration and premature convergence. Alternative neighborhood topologies limit how many other particles each particle can communicate with. For example, particle 5 may only look at itself and particles 4 and 6 when determining what its group best location is. This allows information about high value locations in the domain of the objective function to eventually reach every particle in the swarm, but much more slowly so that each particle has an opportunity to explore the space more fully first. Appendix \ref{subapp:ring} contains a short description of the ring topologies, but there are many alternatives in the literature.[CITATION]

A variant of PSO, the bare bones PSO algorithm (BBPSO) is a PSO algorithm introduced by \citet{kennedy2003bare} that strips away the velocity term and removes the need for the user to choose parameters outside of the swarm size. Let $\theta_{ij}(t)$ denote the $j$th coordinate of the position for the $i$th particle in period $t$, and similarly for $p_{ij}(t)$ and $g_{ij}(t)$. Then the BBPSO algorithm obtains a new position coordinate $\theta_{ij}$ via
\begin{align}\label{eq:bbpso}
\theta_{ij}(t+1) \sim N\left(\frac{p_{ij}(t) + g_{ij}(t)}{2}, |p_{ij}(t) - g_{ij}(t)|^2\right)
\end{align}
where $N(\mu,\sigma^2)$ is the normal distribution with mean $\mu$ and standard deviation $\sigma^2$. The updates of $\bm{p}_i(t)$ and $\bm{g}_i(t)$ are the same as in \eqref{eq:pso}. There are several variants of this algorithm proposed including using distributions from different location-scale families --- e.g., using the $t$-distribution and modifying the location or the scale parameters, for example \citet{krohling2009bare}, \citet{hsieh2010modified}, \citet{richer2006levy}, and \citet{campos2014bare}. Appendix \ref{subapp:bbpso} contains more detail on some of thees BBPSO variants.

\subsection{Adaptively tuned BBPSO}\label{subsec:ATBBPSO}
BBPSO adapts the size effective search space of the swarm over time through the variance term, $|p_{ij}(t) - g_{ij}(t)|^2$. As the personal best locations of the swarm move closer together, these variances decrease and the swarm tries locations which are closer to known high value areas in the space. This behavior is desirable, but the adaptation is forced to occur through one channel: the personal and group best locations. If the personal best locations of the swarm are arranged in a rough ring around the global optimum, smaller variances are desirable so that the new particle locations have a tendency to be in the center of the ring. On the other hand, if the personal best locations of the swarm are all to one side of the optimum and fairly far away, larger variances are desirable so that the particles can quickly approach the neighborhood of the optimum. BBPSO cannot distinguish between these two cases. [CAN ADD A GRAPHIC ILLUSTRATING THIS. WORTH IT?] 

In order to allow it to adapt in a more flexible manner, we modify the BBPSO variance to $\sigma^2(t)|p_{ij}(t) - g_{ij}(t)|^2$ and tune $\sigma^2(t)$ in a manner similar to adaptive random walk Metropolis MCMC algorithms \citep{andrieu2008tutorial}. Define the improvement rate of the swarm in period $t$ as $R(t) = \#\{i:Q(\bm{p}_i(t))> Q(\bm{p}_i(t-1))\}/n$ where if $A$ is a set then $\#A$ is the number of members of that set, and let $R^*$ denote the target improvement rate. Then what we term adaptively tuned BBPSO (AT-BBPSO) updates personal best and group best locations as in \eqref{eq:pso}, then updates particle locations as follows:
\begin{align}\label{eq:at-bbpso}
\theta_{ij}(t+1) &\sim t_{df}\left(\frac{p_{ij}(t) + g_{ij}(t)}{2}, \sigma^2(t)|p_{ij}(t) - g_{ij}(t)|^2\right),\nonumber\\
\log \sigma^2(t+1) &= \log\sigma^2(t) + c\times\{R(t+1) - R^*\},
\end{align}
where $df$ is a user chosen degrees of freedom parameter and $c$ is another user chosen parameter that controls the speed of adaptation. We use a $t$ kernel instead of a Gaussian in order to allow for more flexibility, and smaller values of $df$ appear to combine well with adaptively tuning $\sigma^2(t)$ --- in particular $df=1$. Since the target rate in AT-BBPSO is similar to the target rate in an adaptive random walk Metropolis algorithm, a priori values around $0.25$ and in particular below $0.5$ seem reasonable \citep{gelman1996efficient}. In practice we find that values from $R^*=0.3$ to $R^*=0.5$ tend to yield good AT-BBPSO algorithms. The parameter $c$ controls the speed of adaptation so that larger values of $c$ mean the algorithm adapts $\sigma^2(t)$ faster. We find that $c=0.1$ to be a good value, though anything within an order of magnitude often yields similar results. The initial value $\sigma^2(0)$ also needs to be chosen, though this does not have much impact on the algorithm as long as $c$ is not too small. We use $\sigma^2(0)=1$ to initialize the algorithm at the standard BBPSO algorithm.

Both using a $t$ kernel and adding a fixed scale parameter have been discussed in the BBPSO literature, but as \citet{kennedy2003bare} notes, something about setting $\sigma=1$ is special that causes the algorithm to work well. Another similar BBPSO algorithm in the literature comes from \citet{hsieh2010modified}. They propose a modified version of BBPSO with 
\begin{align*}
\theta_{ij}(t+1) \sim N\left(\omega\frac{p_{ij}(t) + g_{ij}(t)}{2}, \sigma^2|p_{ij}(t) - g_{ij}(t)|^2\right),
\end{align*}
where $\omega\leq 1$ and $\sigma^2\leq 1$ are constriction parameters that are eventually both set to one after enough iterations of the algorithm. The authors suggest dynamically adjusting the constriction parameters in the early stage of the algorithm before they are set to one, but give no suggestion for how to do this. The default BBPSO algorithm essentially sets $\sigma^2(t)=1$ for all $t$ and \citet{hsieh2010modified}'s algorithm deterministically adjusts $\sigma^2(t)$, but our AT-BBPSO algorithm is able to adapt its value on the fly based on local knowledge about the objective function. If too much of the swarm is failing to find new personal best locations, AT-BBPSO proposes new locations closer to known high value areas. If too much of the swarm is improving, AT-BBPSO proposes bolder locations in an effort to make larger improvements. This ability to adapt to local information about the objective function allows AT-BBPSO to more quickly traverse the search space towards the global optimum, though by using local information AT-BBPSO does risk premature convergence to a local optimum. The adaptively tuned component of AT-BBPSO can also be combined with most BBPSO variants, some of which are outlined in Appendix \ref{subapp:bbpso}. In Appendix \ref{app:psocompare} we conduct a simulation study on several test functions that compares AT-BBPSO variants to other PSO and BBPSO variants in order to justify the parameter settings discussed above and demonstrate AT-BBPSO variants are attractive PSO algorithms.

\subsection{Adaptively tuned PSO}\label{sec:AT-PSO}
In AT-BBPSO variants, the parameter $\sigma(t)$ partially controls the effective size of the swarm's search area, and we increase or decrease $\sigma(t)$ and consequently the search area depending on how much of the swarm is finding new personal best locations. In standard PSO there is no direct analogue to $\sigma(t)$, though the inertia parameter, denoted by $\omega$ in \eqref{eq:pso}, is related. It controls the effective size of the swarm's search area by controlling how the magnitude of the velocities evolve over time --- larger values of $\omega$ allow for larger magnitude velocities in future periods. In AT-BBPSO we use an analogy with tuning a random walk Metropolis-Hastings MCMC algorithm in order to build intuition about how to tune $\sigma(t)$. The analogy is much weaker in this case; nonetheless, we allow $\omega(t)$ to be time-varying and use the same mechanism in order to tune it as we did for $\sigma(t)$.


The idea of time-varying $\omega(t)$ has been in the PSO literature for some time. An early suggestion was to set $\omega(0)=0.9$ and deterministically decrease it until it reaches $\omega(T)=0.4$ after the maximum number of iterations allowed \citep{eberhart2000comparing}. In particular, \citet{tuppadung2011comparing} suggest defining $\omega(t)$ via the parameterized inertia weight function
\begin{align}\label{eq:inertiafun}
\omega(t) = \frac{1}{1 + \left(\frac{t}{\alpha}\right)^{\beta}}
\end{align}
where $\alpha$ and $\beta$ are user-defined parameters. Roughly, $\alpha$ controls how low $\omega(t)$ can go and $\beta$ controls how fast it gets there, so $\alpha$ and $\beta$ can be thought of as intercept and slope parameters respectively. The suggestion in \citet{tuppadung2011comparing} is to set $\alpha$ to a small fraction of the total amount of iterations in which the algorithm is allowed to run (e.g., 10\% or 20\%), and set $\beta$ between one and four. We call this type of PSO algorithm deterministic inertia PSO (DI-PSO).

DI-PSO tends to improve on standard PSO if $\omega(t)$'s progression is set appropriately, but it invariably makes using PSO more difficult for the average user. Additionally, depending on the problem, it may be more useful to let the swarm explore the space for more or less iterations, necessitating different progressions of $\omega(t)$. A priori it may not be clear exactly which approach is best for any given problem, so an automatic method is desirable. adaptively tuned PSO (AT-PSO) is just that --- it provides an automatic method to adjust the value of $\omega(t)$ depending on local information obtained by the particle swarm. Formally, AT-PSO updates personal and group best locations as in \eqref{eq:pso} and updates $\omega(t)$ and particle locations as follows:
\begin{align}\label{eq:atpso}
\bm{v}_i(t+1) &= \omega(t+1) \bm{v}_i(t) + \phi_1 \bm{r}_{1i}(t)\circ\{\bm{p}_i(t) - \bm{\theta}_i(t)\} + \phi_2 \bm{r}_{2i}(t)\circ\{\bm{g}_i(t) - \bm{\theta}_i(t)\},\nonumber\\
\bm{\theta}_i(t+1) &= \bm{\theta}_i(t) + \bm{v}_i(t+1),\nonumber\\
\log\omega(t+1)& = \log\omega(t) + c\times\{R(t+1) - R^*\},
\end{align}
where $R(t)$ is the improvement rate of the swarm in iteration $t$, $R^*$ is the target improvement rate, and $c$ controls how much $R(t)$ changes on a per iteration basis. For AT-BBPSO we used an analogy with random walk Metropolis-Hastings algorithms to suggest that a good value for the target improvement rate is smaller than 0.5 but not too small and this turned out to be correct. The analogy does not apply as well here, though we still find in Appendix \ref{app:psocompare} that $R^*=0.3$ or $0.5$ still seems to work well for AT-PSO. The value of $c$ controls the speed of adaptation, and in particular if $c$ is small and $\omega(0)$ is large, AT-PSO can mimic DI-PSO to some extent. This turns out to produce poor AT-PSO algorithms, however. We use $c=0.1$ as a default value and in simulations not reported here, we find that the gains from optimizing $c$ appear to be small. However, very small values like those suggested by an attempt to mimic DI-PSO turn out to cause the algorithm to perform very poorly. 

A major strength of AT-PSO relative to DI-PSO and standard PSO is that AT-PSO can increase $\omega(t)$ when information from the swarm suggests there is an unexplored high value region of the space --- when too much of the swarm is improving on their personal best locations AT-PSO increases $\omega(t)$ until velocities start increasing, the swarm starts exploring a larger amount of the nearby space, and more of the particles fail to find improvements on their personal best. Just like in AT-BBPSO, this mechanism provides a way for the swarm to adapt its behavior on the fly based on local conditions and speed up convergence by allowing the particles that do improve to make larger improvements, but it can also cause premature convergence to a local optimum. While DI-PSO monotonically decreases $\omega(t)$ toward some minimum value, AT-PSO typically oscillates $\omega(t)$ so that the algorithm alternates between exploring and exploiting more, relative to standard PSO. Appendix \ref{app:psocompare} contains an extended simulation study comparing a variety of these PSO and BBPSO algorithms on a suite of test functions that demonstrates some of the behavior detailed above and shows that AT-PSO is an attractive PSO algorithm. 

\section{The Spatial Design Problem}
Suppose we are interested in the latent spatial field of some response variable $Y(\bm{u})$, $\bm{u}\in \mathcal{D}\subseteq \Re^2$. Specifically, we are interested in predicting $Y(\bm{u})$ at a set of target locations $\bm{t}_1, \bm{t}_2, \dots, \bm{t}_{N_t}\in\mathcal{D}$. We have the ability to sample $N$ locations anywhere in $\mathcal{D}$, and we wish to place them in order to optimize the amount of information, in some sense, that we learn about the latent spatial field. This is a bit naieve since typically the sampled points represent fixed monitoring stations and the question is where to put new monitoring stations. Let $\bm{s}_1, \bm{s}_2, \dots, \bm{s}_{N_s}\in\mathcal{D}$ denote the $N_s$ fixed sampling locations and let $\bm{d}_1, \bm{d}_2, \dots, \bm{d}_{N_d}\in\mathcal{D}$ denote the $N_d$ new sampling locations, A.K.A. the design points. Suppose that $Y(\bm{u})$ is a geostatistical process with mean function $\mu(\bm{u})=\bm{x}(\bm{u})'\bm{\beta}$ for some covariate $\bm{x}(\bm{u})$ known at every point in $\mathcal{D}$ and some covariance function $C(\bm{u}, \bm{v})$ for $\bm{u},\bm{v}\in\mathcal{D}$. Not all covariates can be known a priori at every point in the spatial domain, but e.g. covariates that are known functions of the location satisfy this constraint. Once the design points are selected, we observe $Z(\bm{d}_i)$ for $i=1,2,\dots,N_d$ and $Z(\bm{s}_i)$ for $i=1,2,\dots,N_s$ where $Z(\bm{u}) = Y(\bm{u}) + \varepsilon(\bm{u})$ and $\varepsilon(\bm{u})$ is mean zero white noise with variance $\sigma^2_{\varepsilon}$, representing measurement error. Typically $\bm{\beta}$, $\sigma^2_{\varepsilon}$, and $C(.,.)$ are unknown and must be estimated, though for now we will treat them as known perhaps through estimation with previous measurements at the fixed sampling locations. 


To completely specify the problem, then, we need to define an informative design criterion. Intuitively, we can compute the mean square prediction error (MSPE), i.e. the kriging variance, at each of the target locations and optimize some function of these variances. Common choices are to minimize the mean kriging variance or the maximum kriging variance over all target locations. These criteria are naive since they they ignore parameter uncertainty, and taking that into account will often change the optimal design \citep{zimmerman2006optimal}. For now we will treat only the latent spatial field as unknown and discuss minimizing both mean and maximum kriging variance.

\subsection{Simple Kriging}
In simple kriging, $C(.,.)$, $\bm{\beta}$, and $\sigma^2_{\varepsilon}$ are all treated as known. Let $\bm{Z} = [Z(\bm{s}_1), Z(\bm{s}_2), \dots, Z(\bm{s}_{N_s}), Z(\bm{d}_1), Z(\bm{d}_2), \dots, Z(\bm{d}_{N_d}) ]'$, $\bm{X} = [\bm{x}(\bm{s}_1), \bm{x}(\bm{s}_2), \dots, \bm{x}(\bm{s}_{N_s}), \bm{x}(\bm{d}_1), \bm{x}(\bm{d}_2), \dots, \bm{x}(\bm{d}_{N_d})]'$, $\bm{C}_Z = \cov(\bm{Z})$ where $\cov[Z(\bm{u}), Z(\bm{v})] = C(\bm{u},\bm{v}) + \sigma^2_\varepsilon 1(\bm{u} = \bm{v})$, and $\bm{c}_Y(\bm{t}_i) = \cov[Y(\bm{t}_i), \bm{Z}]$ where $\cov[Y(\bm{t}_i), Z(\bm{u})] = C(\bm{t}_0, \bm{u})$. Then the simple kriging predictor of $Y(\bm{t}_i)$ is \citep[Section~4.1.2]{cressie2015statistics}
\begin{align*}
\widehat{Y}_{sk}(\bm{t}_i;\bm{d}) &= \bm{x}(\bm{s}_i)'\bm{\beta} + \bm{c}_Y(\bm{t}_i;\bm{d})'\bm{C}_Z^{-1}(\bm{d})[\bm{Z}(\bm{d}) - \bm{X}(\bm{d})\bm{\beta}]\\
\intertext{ with MSPE }
\sigma_{sk}^{2}(\bm{t}_i;\bm{d}) &= C(\bm{t}_i,\bm{t}_i) - \bm{c}_Y(\bm{t}_i;\bm{d})'\bm{C}_Z^{-1}(\bm{d})\bm{c}_Y(\bm{t}_i;\bm{d}).
\end{align*}
The simple kriging MSPE a function of the design points, $\bm{d}=(\bm{d}_1',\bm{d}_2',\dots,\bm{d}_{N_d}')'$, through $\bm{c}_Y(\bm{t}_i)$ and $\bm{C}_Z^{-1}$, and we have made this explicit in the above equations. Then the mean kriging variance is given by
\begin{align*}
\overline{V}_{sk}(\bm{d}) &= \frac{1}{N_t}\sum_{i}^{N_t}\sigma^2_{sk}(\bm{t}_i;\bm{d})\\
\intertext{while the maximum kriging variance is given by}
V_{sk}^*(\bm{d}) &= \max_{i=1,2\dots,N_t}\sigma^2_{sk}(\bm{t}_i;\bm{d}).
\end{align*}
In practice we are often interested in predicting at the entire spatial domain rather than a finite set of target locations. This changes the mean and maximum kriging variances to
\begin{align*}
\overline{V}_{sk}(\bm{d}) &= \frac{1}{|\mathcal{D}|}\int_{\mathcal{D}}\sigma^2_{sk}(\bm{d};\bm{d})d\bm{u}\\
\intertext{and}
V_{sk}^*(\bm{d}) &= \max_{\bm{u}\in\mathcal{D}}\sigma^2_{sk}(\bm{u};\bm{d}).
\end{align*}
respectively. In practice we can approximate both of these with a large but finite sample of locations from $\mathcal{D}$, though if the sample is too small some design critera may favor points directly on top of the sampled locations .

\subsection{Universal Kriging}
A major limitation of simple kriging is that typically $\bm{\beta}$, $\sigma^2_{\varepsilon}$, and $C(.,.)$ are unknown. Universal kriging attempts to remedy this to some extent by allowing $\bm{\beta}$ to be unknown. The universal kriging predictor of $Y(\bm{t}_i)$ is \citep[Section~4.1.2]{cressie2015statistics}
\begin{align*}
\widehat{Y}_{uk}(\bm{t}_i;\bm{d}) &= \bm{x}(\bm{t}_i)'\widehat{\bm{\beta}}_{gls} + \bm{c}_Y(\bm{t}_i)'\bm{C}_Z^{-1}(\bm{Z} - \bm{X}\widehat{\bm{\beta}}_{gls})\\
\intertext{where $\widehat{\bm{\beta}}_{gls} = [\bm{X}'\bm{C}_Z^{-1}\bm{X}]^{-1}\bm{X}'\bm{C}_Z^{-1}\bm{Z}$ is the generalized least squares estimate of $\bm{\beta}$, and the MSPE of $\widehat{Y}_{uk}(\bm{t}_i)$ is}
\sigma_{uk}^2(\bm{t}_i;\bm{d}) &= C(\bm{t}_i, \bm{t}_i) - \bm{c}_Y(\bm{t}_i)'\bm{C}_Z^{-1}\bm{c}_Y(\bm{t}_i)  \\
& + [\bm{x}(\bm{t}_i)  - \bm{X}'\bm{C}_Z^{-1}\bm{c}_Y(\bm{t}_i)]'[\bm{X}'\bm{C}_Z^{-1}\bm{X}]^{-1}[\bm{x}(\bm{t}_i)  - \bm{X}'\bm{C}_Z^{-1}\bm{c}_Y(\bm{t}_i)].
\end{align*}
To avoid clutter we drop the explicit dependence on $\bm{d}$ in these equations, but $\bm{c}_Y(\bm{t}_i)$, $\bm{C}_Z$, $\bm{Z}$, $\bm{X}$, and $\widehat{\bm{\beta}}_{gls}$ all depend on $\bm{d}$. The mean universal kriging variance is given by
\begin{align*}
\overline{V}_{uk}(\bm{d}) &= \frac{1}{N_t}\sum_{i}^{N_t}\sigma^2_{uk}(\bm{t}_i;\bm{d})\\
\intertext{while the maximum universal kriging variance is given by}
V_{uk}^*(\bm{d}) &= \max_{i=1,2\dots,N_t}\sigma^2_{uk}(\bm{t}_i;\bm{d}).
\end{align*}
In this context, \cite{zimmerman2006optimal} finds that the optimal design is highly dependent on the class of mean function of the geostatistical process. 

\subsection{Empirical Kriging}
In both simple and universal kriging we assume that $\sigma^2_{\varepsilon}$ and $C(.,.)$ are known, though this is not always reasonable. Suppose the covariance function is parameterized by $\bm{\theta}$. A common classical estimation strategy is to estimate $(\bm{\theta}, \sigma^2)$ via maximum likelihood or residual maximum likelihood, then plug these estimates into the universal kriging formulas for prediction. The MSPE of this estimator for $Y(\bm{t}_i)$ is unknown, but \cite{zimmerman2006optimal} discusses some approximations using the Fisher information matrix. Then for the design problem, \cite{zimmerman2006optimal} suggests minimizing either the maximum approximate MSPE of the target locations, or minimizing the mean approximate MSPE.

In the Bayesian context, the standard design criterion is the expected entropy gain from conditioning on the observations \citep{ebrahimi2010information}, defined by $\mathrm{E}\left(\log \frac{[\bm{Y}|\bm{Z}]}{[\bm{Y}]}\right)$ where $\bm{Y}=[Y(\bm{t}_1), Y(\bm{t}_2), \dots, Y(\bm{t}_{N_t})]'$, the expectation is taken over $\bm{Y}$, $\bm{Z}$, and any model parameters, and $[.]$ denotes the (conditional) probability density of the enclosed random variable. For example, \cite{fuentes2007bayesian} uses this criterion. It turns out that in the simple and universal kriging cases, i.e. all parameters known and all parameters but $\bm{\beta}$ known respectively, the entropy approach with a specific prior is equivalent to minimizing the log determinant of the MSPE matrix. The key difference between this and maximizing $\overline{V}$ is that the entropy criterion takes into account the covariance between the prediction errors at the various target locations. We mention both this and universal kriging for completeness, but will restrict ourselves to simple kriging in the example in the next section.

\clearpage\pagebreak\newpage\thispagestyle{empty}
\bibliographystyle{../jasa}
\bibliography{../pso}
\end{document}
