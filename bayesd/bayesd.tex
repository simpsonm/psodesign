\documentclass[12pt]{article}
\usepackage{amsmath, bm, bbm}
\title{A Note on Bayesian D-optimality}
\author{Matt Simpson}
\begin{document}
\maketitle
\section{Introduction}
This is a quick note outlining the class of problems we need to look for in order to use PSO for finding Bayesian D-optimal designs, or nearly optimial designs. Strictly speaking, D-optimality is more narrow than necessary. 
\section{General Framework}
Suppose we wish to design an experiment with $n$ experimental units in order to determine the effect of some covariates on some response $y_i$, $i=1,2,\dots,n$. Let $\bm{y}=\bm{y}_{1:n}=(y_1,y_2,\dots,y_n)'$. The covariates are split into two types --- fixed covariates and adjustable covariates. Fixed covariates are immutable features of the experimental units, while adjustable covariates are features which are chosen by the experimenter. For example, any treatment applied to the experimental unity is an adjustable covariate. Let $\bm{X}^f$ denote the $n\times p$ matrix of fixed covariates, $\bm{X}^a$ denote the $n\times q$ matrix of adjustable covariates, and $\bm{X}=[\bm{X}^f, \bm{X}^a]$ the full $n\times(p+q)$ matrix of covariates. Finally let $\bm{\theta}$ denote a vector of model parameters. Then the generic model is
\begin{align*}
\bm{y}|\bm{X},\bm{\theta} &\sim [\bm{y}|\bm{X},\bm{\theta}], &&& \bm{\theta}|\bm{X}&\sim [\bm{\theta}]
\end{align*}
where $[.|.]$ denotes the probability density or mass function of the enclosed random variables. Often $\bm{\theta}=(\bm{\beta},\phi)$, $\bm{y}|\bm{X},\bm{\theta}\sim [\bm{y}|\bm{\mu},\phi]$ with $g(\bm{\mu}) = \bm{X}\bm{\beta}$ and $g(.)$ is a known link function.

The design problem is to choose $\bm{X}^a \in \mathcal{X}$ in order to maximize some design criterion. The space $\mathcal{X}$ contains all constraints on the elements of $\bm{X}^a$, for example if $x_{i2} = x_{i1}^2$ in a quadratic regression model. What is being chosen here is not which covariates to include, but rather the values of the covariates. For example in a simple linear regression model with $\bm{X}^f = (1,1,\dots,1)'$ correpsonding to the intercept and $\bm{X}^a=(x_1,x_2,\dots,x_n)$ corresponding to the slope, the design problem is to choose the values of $x_1, x_2, \dots, x_n$ for a fixed $n$, typically where $x_i$ lives in a constrained space.

A standard Bayesian design criterion is the expected shannon information gain:
\begin{align*}
\mathrm{E}_{\bm{y},\bm{\theta}}\left\{\log \frac{[\bm{\theta}|\bm{y},\bm{X}]}{[\bm{\theta}]}\right\}    = \iint \log \frac{[\bm{\theta}|\bm{y},\bm{X}]}{[\bm{\theta}]} [\bm{y}|\bm{X},\bm{\theta}][\bm{\theta}] d\bm{\theta} d\bm{y}.
\end{align*}
Maximizing this in $\bm{X}^a$ is equivalent to maximizing
\begin{align*}
U(\bm{X}^a) = \iint \log \frac{[\bm{y}|\bm{X},\bm{\theta}]}{[\bm{y}|\bm{X}]} [\bm{y}|\bm{X},\bm{\theta}][\bm{\theta}] d\bm{\theta} d\bm{y}.
\end{align*}
When $\bm{\theta}\sim N(\bm{\theta}_0, \sigma^2\bm{S}_0)$ and $\bm{y}|\bm{X},\bm{\theta} \sim N(\bm{X}\bm{\theta}, \sigma^2\bm{I}_n)$ where $\bm{I}_n$ is the $n\times n$ identity matrix and $\sigma^2$ is known, then maximizing $U(\bm{X}^a)$ is equivalent to maximizing the Bayesian D-optimality criterion:
\begin{align*}
D(\bm{X}^{a}) = |\bm{X}'\bm{X} + \bm{S}_0^{-1}|.
\end{align*}
However in general, maximizing $U$ and $D$ are not equivalent, and when they are not, typically $U(\bm{X}^a)$ must be approximated via monte carlo simulation as well as an approximation for the model's marginal likelihood, $[\bm{y}|\bm{X}]$.

Whether the model is $U$ or $D$, finding the optimal design is a difficult optimization problem, and near-optimal designs are often desireable. As such, this is a perfect problem for heuristic optimization algorithms such as particle swarm optimization. The problem for us, then, is to find an example that fits into this framework and ideally uses federal data.

\section{Bayesian D-Optimality for Choice Experiments}

A wide range of fields from economics to marketing use discrete choice experiments in order to evaluate the preferences of individuals about various items in a choice set. Economists might be interested in, for example, nonmarket valuation --- e.g. attaching a dollar value to environmental benefits that do not have a market price, while marketers might be interesting in finding the optimal product mix in some industry.

The discrete choice model framework these experiments use is as follows. Let $i=1,2,\dots,I$ indicate the individual choosers, $j=1,2,\dots,J_i$ indicate the choice sets that chooser $i$ sees, and $k=1,2,\dots,K_{ij}$ indicate the options available to chooser $i$ in choice set $j$. Then we represent the utility of option $k$ of set $j$ for individual $i$ as $u_{ijk} = \bm{x}'_{ijk}\bm{\beta} + \varepsilon_{ijk}$ Here $\bm{x}'_{ijk}$ is a vector of covariates containing information about the option available to the chooser in this choice, and also possibly containing information about the chooser such as demographics. Typically $\varepsilon_{ijk}\stackrel{iid}{\sim} F$ where $F$ is some known cdf, often the Gumbel cdf, or the normal cdf. Especially in the case of a normal cdf, the independence assumption can be relaxed though this can cause identifiability issues. The utility of each option is unobserved, but option $i$ chooses is observed. If option $k^*$ is chosen, this implies that $u_{ijk^*} = \max_{k}u_{ijk}$. Under $iid$ Gumbel errors, the probability that option $k$ is chosen is given by
\begin{align*}
p_{ijk} = \frac{e^{\bm{x}'_{ijk}\bm{\beta}}}{\sum_{t=1}^{K_{ij}}e^{\bm{x}_{ijt}'\bm{\beta}}},
\end{align*}
which gives rise to the multinomial logit (MNL) model. Let $y_{ijk}=1$ if chooser $i$ chooses option $k$ of set $j$, and $y_{ijk}=0$ if they choose any other option in the set. Then letting $\bm{y}_{ij}=(y_{ij1},\dots,y_{ijK_{ij}})$ and similarly for $\bm{p}_{ij}$, we have $\bm{y}_{ij}\stackrel{ind}{\sim} \mathrm{Multinomial}(1, \bm{p}_{ij})$. 

% In reality individual consumers often face many possible choices at once, but in discrete choice {\it experiments} it is often preferable to provide choosers with only two options at one time in order to reduce cognitive load. This allows us to simplify the model considerably. Now let $y_{ij} = 1$ if chooser $i$ in choice $j$ chose the first option, and $y_{ij}$ if they chose the second option (where ``first'' and ``second'' are arbitrarily chosen). Further let $\bm{x}_{ij}'$ be a vector of covariates with information about both options as well as potentially the chooser. Then $p_{ij}=1/(1 + e^{-\bm{x}_{ij}'\bm{\beta}})$ is the probability that the first option is chosen by chooser $i$ in choice $j$ and the model can be written as $y_{ij}\stackrel{ind}{\sim}\mathrm{Bernoulli}(p_{ij})$. We will focus on this relatively simple logistic regression model, though the design methodology applies more generally.

The problem, then, is to choose which choice sets are presented to which choosers in order to maximize some design criterion, typically in order to maximize the information learned about the parameter $\bm{\beta}$. Most design criterion used in the literature are based on the Fisher information (FI) matrix. The FI matrix can be written as
\begin{align*}
I(\bm{X},\bm{\beta}) = -\sum_{i=1}^I\sum_{j=1}^{J_i}\bm{X}_{ij}'(\bm{P}_{ij} - \bm{p}_{ij}\bm{p}_{ij}')\bm{X}_{ij}
\end{align*}
where $\bm{X}_{ij} = (\bm{x}_{ij1}, \bm{x}_{ij2}, \dots, \bm{x}_{ijK_{ij}})'$, $\bm{p}_{ij} = (p_{ij1}, p_{ij2}, \dots, p_{ijK_{ij}})'$, and $\bm{P}_{ij}=\mathrm{diag}(\bm{p}_{ij})$. The matrix does not depend on the values of the $y_{ijk}$s, so the expected information matrix is the same. Typically the goal is to maximize $\mathrm{det}[-I(\bm{X},\bm{\beta})]$ or some function of it. The FI matrix is defined for a specfic value of $\bm{\beta}$, so there are two approaches: choose a reasonable value of $\bm{\beta}$ and find the optimal design for that particular value, or put a prior on $\bm{\beta}$ and maximize the expected value of the FI with respect to that prior. A Bayesian entropy based design criterion can also be used, though I do not think this corresponds to maximizing the a priori expected FI.

The main problem with these design problems, from our perspective, is that the choice sets live in a discrete space. Typically it involves varying one or more factors on a finite number of levels. Any continuous factors, such as price, are reduced to a small number of possible factors. So if we use this sort example, we might not have a ready-made example.
\end{document}